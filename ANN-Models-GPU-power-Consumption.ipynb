{"cells":[{"cell_type":"markdown","id":"fc8005dd","metadata":{"id":"fc8005dd"},"source":["# ANN- Univariate Time Series Solar Radiation(GHI) Forecasting with MLP, CNN, and LSTM Networks"]},{"cell_type":"markdown","id":"a6db958e-c7fb-4950-8406-b3fe5b41a604","metadata":{"id":"a6db958e-c7fb-4950-8406-b3fe5b41a604"},"source":["# MLP"]},{"cell_type":"code","source":["import torch\n","\n","# Check if GPU is available\n","print(\"CUDA Available:\", torch.cuda.is_available())\n","\n","# Check the name of the GPU\n","if torch.cuda.is_available():\n","    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7n1uf3o5wTt7","executionInfo":{"status":"ok","timestamp":1736798420731,"user_tz":-120,"elapsed":6378,"user":{"displayName":"bahgat ayasi","userId":"01883633993023881350"}},"outputId":"9c5fdc05-d334-4d7b-d17c-7ae6dc4037da"},"id":"7n1uf3o5wTt7","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA Available: True\n","GPU Name: Tesla T4\n"]}]},{"cell_type":"code","source":["!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7tdnVoswfhH","executionInfo":{"status":"ok","timestamp":1736798499822,"user_tz":-120,"elapsed":3364,"user":{"displayName":"bahgat ayasi","userId":"01883633993023881350"}},"outputId":"624c888d-e871-42b1-8ca9-72915ae9dfd3"},"id":"a7tdnVoswfhH","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://download.pytorch.org/whl/cu118\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"]}]},{"cell_type":"code","execution_count":4,"id":"81d57f75-b81b-41fc-b63b-a074c4fefe13","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"81d57f75-b81b-41fc-b63b-a074c4fefe13","executionInfo":{"status":"ok","timestamp":1736799517404,"user_tz":-120,"elapsed":56853,"user":{"displayName":"bahgat ayasi","userId":"01883633993023881350"}},"outputId":"29c3dc6c-1bc6-4f1e-feda-27bd525a8cea"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-21e13144e22b>:50: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  data = pd.read_csv('jordan_pv.csv', parse_dates=['date'], index_col='date')\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Cleared existing log file: training_power_log.csv\n","Starting training and logging power consumption...\n","Epoch 1/60, Train Loss: 0.014148, Val Loss: 0.001350\n","Epoch 2/60, Train Loss: 0.004043, Val Loss: 0.001153\n","Epoch 3/60, Train Loss: 0.003589, Val Loss: 0.001363\n","Epoch 4/60, Train Loss: 0.003438, Val Loss: 0.001083\n","Epoch 5/60, Train Loss: 0.003368, Val Loss: 0.001042\n","Epoch 6/60, Train Loss: 0.003269, Val Loss: 0.001123\n","Epoch 7/60, Train Loss: 0.003242, Val Loss: 0.001035\n","Epoch 8/60, Train Loss: 0.003170, Val Loss: 0.000970\n","Epoch 9/60, Train Loss: 0.003160, Val Loss: 0.000874\n","Epoch 10/60, Train Loss: 0.003076, Val Loss: 0.000667\n","Epoch 11/60, Train Loss: 0.003076, Val Loss: 0.000970\n","Epoch 12/60, Train Loss: 0.002962, Val Loss: 0.000631\n","Epoch 13/60, Train Loss: 0.003008, Val Loss: 0.000714\n","Epoch 14/60, Train Loss: 0.003000, Val Loss: 0.000530\n","Epoch 15/60, Train Loss: 0.002979, Val Loss: 0.000579\n","Epoch 16/60, Train Loss: 0.002972, Val Loss: 0.000719\n","Epoch 17/60, Train Loss: 0.002935, Val Loss: 0.001329\n","Epoch 18/60, Train Loss: 0.002913, Val Loss: 0.000616\n","Epoch 19/60, Train Loss: 0.002907, Val Loss: 0.000474\n","Epoch 20/60, Train Loss: 0.002891, Val Loss: 0.000884\n","Epoch 21/60, Train Loss: 0.002900, Val Loss: 0.000690\n","Epoch 22/60, Train Loss: 0.002882, Val Loss: 0.000659\n","Epoch 23/60, Train Loss: 0.002847, Val Loss: 0.000613\n","Epoch 24/60, Train Loss: 0.002843, Val Loss: 0.000453\n","Epoch 25/60, Train Loss: 0.002850, Val Loss: 0.000695\n","Epoch 26/60, Train Loss: 0.002760, Val Loss: 0.000524\n","Epoch 27/60, Train Loss: 0.002812, Val Loss: 0.000459\n","Epoch 28/60, Train Loss: 0.002831, Val Loss: 0.000440\n","Epoch 29/60, Train Loss: 0.002825, Val Loss: 0.000526\n","Epoch 30/60, Train Loss: 0.002801, Val Loss: 0.000896\n","Epoch 31/60, Train Loss: 0.002797, Val Loss: 0.000576\n","Epoch 32/60, Train Loss: 0.002762, Val Loss: 0.000558\n","Epoch 33/60, Train Loss: 0.002766, Val Loss: 0.000850\n","Epoch 34/60, Train Loss: 0.002782, Val Loss: 0.000905\n","Epoch 35/60, Train Loss: 0.002749, Val Loss: 0.000508\n","Epoch 36/60, Train Loss: 0.002721, Val Loss: 0.000868\n","Epoch 37/60, Train Loss: 0.002746, Val Loss: 0.000710\n","Epoch 38/60, Train Loss: 0.002739, Val Loss: 0.000511\n","Epoch 39/60, Train Loss: 0.002711, Val Loss: 0.000749\n","Epoch 40/60, Train Loss: 0.002733, Val Loss: 0.000634\n","Epoch 41/60, Train Loss: 0.002736, Val Loss: 0.000730\n","Epoch 42/60, Train Loss: 0.002719, Val Loss: 0.000530\n","Epoch 43/60, Train Loss: 0.002738, Val Loss: 0.000917\n","Epoch 44/60, Train Loss: 0.002689, Val Loss: 0.000543\n","Epoch 45/60, Train Loss: 0.002654, Val Loss: 0.000590\n","Epoch 46/60, Train Loss: 0.002744, Val Loss: 0.000823\n","Epoch 47/60, Train Loss: 0.002706, Val Loss: 0.000486\n","Epoch 48/60, Train Loss: 0.002685, Val Loss: 0.000619\n","Epoch 49/60, Train Loss: 0.002714, Val Loss: 0.000644\n","Epoch 50/60, Train Loss: 0.002685, Val Loss: 0.000652\n","Epoch 51/60, Train Loss: 0.002682, Val Loss: 0.000544\n","Epoch 52/60, Train Loss: 0.002653, Val Loss: 0.000729\n","Epoch 53/60, Train Loss: 0.002679, Val Loss: 0.000505\n","Epoch 54/60, Train Loss: 0.002658, Val Loss: 0.000460\n","Epoch 55/60, Train Loss: 0.002657, Val Loss: 0.000596\n","Epoch 56/60, Train Loss: 0.002695, Val Loss: 0.000855\n","Epoch 57/60, Train Loss: 0.002660, Val Loss: 0.000728\n","Epoch 58/60, Train Loss: 0.002695, Val Loss: 0.000789\n","Epoch 59/60, Train Loss: 0.002674, Val Loss: 0.000541\n","Epoch 60/60, Train Loss: 0.002593, Val Loss: 0.000558\n","Training complete. Training time: 50.38 seconds.\n","Calculating power consumption for training...\n","Log file content:\n","  Power (W)\n","0   32.05 W\n","1   32.24 W\n","2   32.24 W\n","3   32.24 W\n","4   32.24 W\n","Cleaned log data:\n","   Power (W)\n","0      32.05\n","1      32.24\n","2      32.24\n","3      32.24\n","4      32.24\n","Training Energy Consumption: 1652.19 Joules (0.000459 kWh)\n","Cleared existing log file: inference_power_log.csv\n","Starting inference and logging power consumption...\n","Inference complete. Calculating power consumption...\n","Log file content:\n","  Power (W)\n","0   32.44 W\n","1   32.34 W\n","Cleaned log data:\n","   Power (W)\n","0      32.44\n","1      32.34\n","Inference Energy Consumption: 64.78 Joules (0.000018 kWh)\n","Unnormalized MSE: 663.836182\n","Unnormalized RMSE: 25.765019\n","Unnormalized MAE: 18.316635\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","import subprocess\n","import time\n","import os\n","\n","# Function to monitor power usage with nvidia-smi\n","def monitor_power(output_file):\n","    # Run nvidia-smi in the background to log power usage\n","    command = f\"nvidia-smi --query-gpu=power.draw --format=csv -l 1 > {output_file}\"\n","    process = subprocess.Popen(command, shell=True)\n","    return process\n","\n","# Function to calculate energy consumption from the log\n","def calculate_energy_consumption(log_file):\n","    # Load the power log file\n","    data = pd.read_csv(log_file, skiprows=1, names=[\"Power (W)\"])\n","\n","    # Debug: Check log content\n","    print(f\"Log file content:\\n{data.head()}\")\n","\n","    # Clean the \"Power (W)\" column to remove non-numeric characters (e.g., \" W\")\n","    data[\"Power (W)\"] = data[\"Power (W)\"].str.extract(r'(\\d+\\.\\d+|\\d+)').astype(float)\n","\n","    # Debug: Check cleaned data\n","    print(f\"Cleaned log data:\\n{data.head()}\")\n","\n","    # Calculate total energy in Joules (assuming logs are every 1 second)\n","    energy_joules = data[\"Power (W)\"].sum()  # Energy = Power × Time (1 second per log)\n","\n","    # Convert total energy to kWh\n","    energy_kwh = energy_joules / (3600 * 1000)\n","\n","    return energy_joules, energy_kwh\n","\n","\n","# Add this at the beginning of your script or before starting power monitoring\n","def clear_log_file(log_file):\n","    if os.path.exists(log_file):\n","        os.remove(log_file)\n","        print(f\"Cleared existing log file: {log_file}\")\n","\n","# Load and preprocess the dataset\n","data = pd.read_csv('jordan_pv.csv', parse_dates=['date'], index_col='date')\n","#data = pd.read_csv('Palestine-PV.csv', parse_dates=['date'], index_col='date')\n","\n","data.index = pd.to_datetime(data.index)\n","data = data.sort_index()\n","data = data.iloc[:, 5].dropna().astype('float32')\n","\n","# Normalize the data\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n","\n","# Split data into train and test sets\n","train_size = int(len(scaled_data) * 0.80)\n","train, test = scaled_data[:train_size], scaled_data[train_size:]\n","\n","# Create sequences for LSTM\n","def to_sequences(dataset, seq_size=1):\n","    x, y = [], []\n","    for i in range(len(dataset) - seq_size - 1):\n","        x.append(dataset[i:(i + seq_size), 0])\n","        y.append(dataset[i + seq_size, 0])\n","    return np.array(x), np.array(y)\n","\n","seq_size = 3  # Number of time steps to look back\n","train_X, train_y = to_sequences(train, seq_size)\n","test_X, test_y = to_sequences(test, seq_size)\n","\n","# Convert to PyTorch tensors\n","train_X = torch.tensor(train_X).float()\n","train_y = torch.tensor(train_y).view(-1, 1).float()\n","test_X = torch.tensor(test_X).float()\n","test_y = torch.tensor(test_y).view(-1, 1).float()\n","\n","# Define the model\n","class NeuralNetwork(nn.Module):\n","    def __init__(self, input_dim, neurons, layers1, activation, dropout_rate):\n","        super(NeuralNetwork, self).__init__()\n","        self.layers = nn.ModuleList()\n","        self.layers.append(nn.Linear(input_dim, neurons))\n","        self.layers.append(getattr(nn, activation)())\n","        for _ in range(layers1):\n","            self.layers.append(nn.Linear(neurons, neurons))\n","            self.layers.append(getattr(nn, activation)())\n","            if dropout_rate > 0:\n","                self.layers.append(nn.Dropout(p=dropout_rate))\n","        self.layers.append(nn.Linear(neurons, 1))\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","# Set up the device (GPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# Initialize the model, loss function, and optimizer\n","activation = 'ReLU'\n","neurons = 32\n","layers1 = 2\n","dropout_rate = 0.05\n","input_dim = train_X.shape[1]\n","learning_rate = 0.001\n","\n","model = NeuralNetwork(input_dim, neurons, layers1, activation, dropout_rate).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Prepare the data loaders\n","batch_size = 64\n","train_dataset = TensorDataset(train_X, train_y)\n","test_dataset = TensorDataset(test_X, test_y)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Train the model and monitor power consumption\n","epochs = 60\n","train_losses, val_losses = [], []\n","min_val_loss = float('inf')\n","\n","# Before starting training power monitoring\n","clear_log_file(training_power_log)\n","\n","print(\"Starting training and logging power consumption...\")\n","training_power_log = \"training_power_log.csv\"\n","monitor_process = monitor_power(training_power_log)\n","\n","# ---- Add training time measurement ----\n","start_time = time.time()\n","\n","for epoch in range(epochs):\n","    model.train()\n","    epoch_loss = 0\n","    for batch_X, batch_y in train_loader:\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(batch_X)\n","        loss = criterion(outputs, batch_y)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for val_X, val_y in test_loader:\n","            val_X, val_y = val_X.to(device), val_y.to(device)\n","            val_outputs = model(val_X)\n","            val_loss += criterion(val_outputs, val_y).item()\n","    val_loss /= len(test_loader)\n","    train_losses.append(epoch_loss / len(train_loader))\n","    val_losses.append(val_loss)\n","    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {epoch_loss / len(train_loader):.6f}, Val Loss: {val_loss:.6f}\")\n","\n","    if val_loss < min_val_loss:\n","        min_val_loss = val_loss\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","# Stop the power monitoring for training\n","end_time = time.time()\n","monitor_process.terminate()\n","monitor_process.wait()\n","\n","training_time = end_time - start_time\n","print(f\"Training complete. Training time: {training_time:.2f} seconds.\")\n","print(\"Calculating power consumption for training...\")\n","\n","# Calculate energy consumption for training\n","train_energy_joules, train_energy_kwh = calculate_energy_consumption(training_power_log)\n","print(f\"Training Energy Consumption: {train_energy_joules:.2f} Joules ({train_energy_kwh:.6f} kWh)\")\n","\n","\n","# Before starting inference power monitoring\n","clear_log_file(inference_power_log)\n","\n","\n","# Inference and monitor power consumption\n","inference_power_log = \"inference_power_log.csv\"\n","print(\"Starting inference and logging power consumption...\")\n","monitor_process = monitor_power(inference_power_log)\n","\n","# Add delay to ensure logging starts\n","time.sleep(1)\n","\n","model.eval()\n","with torch.no_grad():\n","    for _ in range(1):  # Artificially increase inference workload\n","        predictions = model(test_X.to(device)).cpu().numpy()\n","\n","# Add delay to ensure all logs are written\n","time.sleep(1)\n","\n","# Stop the power monitoring for inference\n","monitor_process.terminate()\n","monitor_process.wait()\n","print(\"Inference complete. Calculating power consumption...\")\n","\n","# Calculate energy consumption for inference\n","inference_energy_joules, inference_energy_kwh = calculate_energy_consumption(inference_power_log)\n","print(f\"Inference Energy Consumption: {inference_energy_joules:.2f} Joules ({inference_energy_kwh:.6f} kWh)\")\n","\n","# Rescale predictions and evaluate\n","predictions_unnorm = scaler.inverse_transform(predictions)\n","test_y_unnorm = scaler.inverse_transform(test_y.numpy())\n","\n","mse = mean_squared_error(test_y_unnorm, predictions_unnorm)\n","rmse = np.sqrt(mse)\n","mae = mean_absolute_error(test_y_unnorm, predictions_unnorm)\n","\n","print(f\"Unnormalized MSE: {mse:.6f}\")\n","print(f\"Unnormalized RMSE: {rmse:.6f}\")\n","print(f\"Unnormalized MAE: {mae:.6f}\")\n"]},{"cell_type":"code","source":["!nvidia-smi\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YpziHubTxQoY","executionInfo":{"status":"ok","timestamp":1736426734502,"user_tz":-120,"elapsed":267,"user":{"displayName":"bahgat ayasi","userId":"01883633993023881350"}},"outputId":"70d0cf18-8111-4a36-f344-6eba6cdf04a1"},"id":"YpziHubTxQoY","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jan  9 12:45:34 2025       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rVY8x_RRxnfY"},"id":"rVY8x_RRxnfY","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"m_kv54F9xnkA"},"id":"m_kv54F9xnkA","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"c816a481-78f5-481c-b62b-8159d317a294","metadata":{"id":"c816a481-78f5-481c-b62b-8159d317a294","outputId":"121e2709-ac36-4abd-be6a-8beb32b76f8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["NeuralNetwork(\n","  (layers): ModuleList(\n","    (0): Linear(in_features=3, out_features=32, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=32, out_features=32, bias=True)\n","    (3): ReLU()\n","    (4): Dropout(p=0.05, inplace=False)\n","    (5): Linear(in_features=32, out_features=32, bias=True)\n","    (6): ReLU()\n","    (7): Dropout(p=0.05, inplace=False)\n","    (8): Linear(in_features=32, out_features=1, bias=True)\n","  )\n",")\n"]}],"source":["print(model)"]},{"cell_type":"markdown","id":"f4886c4a","metadata":{"id":"f4886c4a"},"source":["# CNN"]},{"cell_type":"code","execution_count":6,"id":"55983fbd-3082-453f-8b41-4608fbaa8aea","metadata":{"id":"55983fbd-3082-453f-8b41-4608fbaa8aea","outputId":"52d429ea-68e0-465f-c36e-c2ad9a049ce3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736799827503,"user_tz":-120,"elapsed":68639,"user":{"displayName":"bahgat ayasi","userId":"01883633993023881350"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-99d867dcf0f5>:52: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  data = read_csv('jordan_pv.csv', parse_dates=['date'], index_col='date')\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Trainable parameters: 6369\n","Cleared existing log file: training_power_log.csv\n","Starting training and logging power consumption...\n","Epoch 1/60, Train Loss: 0.013092, Val Loss: 0.001701\n","Epoch 2/60, Train Loss: 0.004034, Val Loss: 0.000992\n","Epoch 3/60, Train Loss: 0.003650, Val Loss: 0.003101\n","Epoch 4/60, Train Loss: 0.003422, Val Loss: 0.000990\n","Epoch 5/60, Train Loss: 0.003404, Val Loss: 0.001139\n","Epoch 6/60, Train Loss: 0.003214, Val Loss: 0.000734\n","Epoch 7/60, Train Loss: 0.003211, Val Loss: 0.000893\n","Epoch 8/60, Train Loss: 0.003165, Val Loss: 0.000614\n","Epoch 9/60, Train Loss: 0.003171, Val Loss: 0.000891\n","Epoch 10/60, Train Loss: 0.003081, Val Loss: 0.000648\n","Epoch 11/60, Train Loss: 0.003055, Val Loss: 0.000745\n","Epoch 12/60, Train Loss: 0.003105, Val Loss: 0.001131\n","Epoch 13/60, Train Loss: 0.003059, Val Loss: 0.000685\n","Epoch 14/60, Train Loss: 0.003022, Val Loss: 0.001005\n","Epoch 15/60, Train Loss: 0.003019, Val Loss: 0.000635\n","Epoch 16/60, Train Loss: 0.002942, Val Loss: 0.001400\n","Epoch 17/60, Train Loss: 0.002926, Val Loss: 0.000624\n","Epoch 18/60, Train Loss: 0.002872, Val Loss: 0.000927\n","Epoch 19/60, Train Loss: 0.002843, Val Loss: 0.001535\n","Epoch 20/60, Train Loss: 0.002838, Val Loss: 0.000977\n","Epoch 21/60, Train Loss: 0.002796, Val Loss: 0.000607\n","Epoch 22/60, Train Loss: 0.002854, Val Loss: 0.000832\n","Epoch 23/60, Train Loss: 0.002837, Val Loss: 0.000668\n","Epoch 24/60, Train Loss: 0.002779, Val Loss: 0.001565\n","Epoch 25/60, Train Loss: 0.002820, Val Loss: 0.000641\n","Epoch 26/60, Train Loss: 0.002754, Val Loss: 0.001424\n","Epoch 27/60, Train Loss: 0.002727, Val Loss: 0.000988\n","Epoch 28/60, Train Loss: 0.002770, Val Loss: 0.000707\n","Epoch 29/60, Train Loss: 0.002761, Val Loss: 0.000701\n","Epoch 30/60, Train Loss: 0.002686, Val Loss: 0.000787\n","Epoch 31/60, Train Loss: 0.002775, Val Loss: 0.001055\n","Epoch 32/60, Train Loss: 0.002754, Val Loss: 0.001006\n","Epoch 33/60, Train Loss: 0.002716, Val Loss: 0.000668\n","Epoch 34/60, Train Loss: 0.002696, Val Loss: 0.001280\n","Epoch 35/60, Train Loss: 0.002721, Val Loss: 0.000765\n","Epoch 36/60, Train Loss: 0.002674, Val Loss: 0.000590\n","Epoch 37/60, Train Loss: 0.002696, Val Loss: 0.000444\n","Epoch 38/60, Train Loss: 0.002664, Val Loss: 0.000589\n","Epoch 39/60, Train Loss: 0.002692, Val Loss: 0.000660\n","Epoch 40/60, Train Loss: 0.002689, Val Loss: 0.000492\n","Epoch 41/60, Train Loss: 0.002712, Val Loss: 0.001010\n","Epoch 42/60, Train Loss: 0.002743, Val Loss: 0.000485\n","Epoch 43/60, Train Loss: 0.002657, Val Loss: 0.000522\n","Epoch 44/60, Train Loss: 0.002714, Val Loss: 0.000579\n","Epoch 45/60, Train Loss: 0.002694, Val Loss: 0.000665\n","Epoch 46/60, Train Loss: 0.002707, Val Loss: 0.000596\n","Epoch 47/60, Train Loss: 0.002656, Val Loss: 0.001323\n","Epoch 48/60, Train Loss: 0.002640, Val Loss: 0.001467\n","Epoch 49/60, Train Loss: 0.002665, Val Loss: 0.001110\n","Epoch 50/60, Train Loss: 0.002633, Val Loss: 0.000454\n","Epoch 51/60, Train Loss: 0.002671, Val Loss: 0.000482\n","Epoch 52/60, Train Loss: 0.002663, Val Loss: 0.000933\n","Epoch 53/60, Train Loss: 0.002611, Val Loss: 0.000834\n","Epoch 54/60, Train Loss: 0.002624, Val Loss: 0.000632\n","Epoch 55/60, Train Loss: 0.002621, Val Loss: 0.000559\n","Epoch 56/60, Train Loss: 0.002579, Val Loss: 0.000988\n","Epoch 57/60, Train Loss: 0.002604, Val Loss: 0.000868\n","Epoch 58/60, Train Loss: 0.002629, Val Loss: 0.000465\n","Epoch 59/60, Train Loss: 0.002602, Val Loss: 0.001449\n","Epoch 60/60, Train Loss: 0.002591, Val Loss: 0.000464\n","Training complete. Calculating power consumption...\n","Training Time: 64.48 sec\n","Training Energy Consumption: 2109.70 Joules (0.000586 kWh)\n","Cleared existing log file: inference_power_log.csv\n","Starting inference and logging power consumption...\n","Inference Energy Consumption: 64.28 Joules (0.000018 kWh)\n","Final Unnormalized Metrics (on last inference results):\n"," MSE:  553.537109\n"," RMSE: 23.527369\n"," MAE:  12.814502\n","Best Validation Loss (MSE): 0.000444\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import time\n","from sklearn.preprocessing import MinMaxScaler\n","from pandas import read_csv\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","import subprocess\n","\n","# -----------------------------------------------------------------------\n","# 1) Functions for GPU Power Monitoring, as in the second script\n","# -----------------------------------------------------------------------\n","def monitor_power(output_file):\n","    \"\"\"\n","    Run nvidia-smi in the background to log power usage (in W) every 1 second.\n","    \"\"\"\n","    command = f\"nvidia-smi --query-gpu=power.draw --format=csv -l 1 > {output_file}\"\n","    process = subprocess.Popen(command, shell=True)\n","    return process\n","\n","def calculate_energy_consumption(log_file):\n","    \"\"\"\n","    Parse the CSV log with GPU power usage and calculate:\n","      - total energy in Joules\n","      - total energy in kWh\n","    \"\"\"\n","    data = pd.read_csv(log_file, skiprows=1, names=[\"Power (W)\"])\n","\n","    # Clean the \"Power (W)\" column in case it contains the units\n","    data[\"Power (W)\"] = data[\"Power (W)\"].str.extract(r'(\\d+\\.\\d+|\\d+)').astype(float)\n","\n","    # Each reading is 1 second apart, so total Joules is sum(power_in_watts) * 1 second\n","    energy_joules = data[\"Power (W)\"].sum()\n","\n","    # Convert Joules to kWh\n","    energy_kwh = energy_joules / (3600 * 1000)\n","\n","    return energy_joules, energy_kwh\n","# Add this at the beginning of your script or before starting power monitoring\n","def clear_log_file(log_file):\n","    if os.path.exists(log_file):\n","        os.remove(log_file)\n","        print(f\"Cleared existing log file: {log_file}\")\n","# -----------------------------------------------------------------------\n","# 2) Data Loading and Preprocessing (same as your original first script)\n","# -----------------------------------------------------------------------\n","# Load and preprocess the dataset\n","data = read_csv('jordan_pv.csv', parse_dates=['date'], index_col='date')\n","#data = read_csv('Palestine-PV.csv', parse_dates=['date'], index_col='date')\n","\n","# Convert index to datetime and sort\n","data.index = pd.to_datetime(data.index)\n","data = data.sort_index()\n","\n","# Select the 6th column (index 5) for the target variable, remove NaN\n","data = data.iloc[:, 5].dropna().astype('float32')\n","\n","# Convert to np.array and scale\n","data = np.array(data).reshape(-1, 1)\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled_data = scaler.fit_transform(data)\n","\n","# Train/test split\n","train_size = int(len(scaled_data) * 0.80)\n","train, test = scaled_data[0:train_size, :], scaled_data[train_size:len(scaled_data), :]\n","\n","def to_sequences(dataset, seq_size=1):\n","    x, y = [], []\n","    for i in range(len(dataset) - seq_size - 1):\n","        window = dataset[i:(i + seq_size), 0]\n","        x.append(window)\n","        y.append(dataset[i + seq_size, 0])\n","    return np.array(x), np.array(y)\n","\n","seq_size = 3\n","train_X, train_y = to_sequences(train, seq_size)\n","test_X, test_y = to_sequences(test, seq_size)\n","\n","# Convert to Torch tensors\n","train_X = torch.tensor(train_X).float()\n","train_y = torch.tensor(train_y).view(-1, 1).float()\n","test_X = torch.tensor(test_X).float()\n","test_y = torch.tensor(test_y).view(-1, 1).float()\n","\n","# -----------------------------------------------------------------------\n","# 3) Define the Model\n","# -----------------------------------------------------------------------\n","class NeuralNetwork(nn.Module):\n","    def __init__(self, input_dim, neurons, layers1, activation, dropout_rate):\n","        super(NeuralNetwork, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 32, kernel_size=1)\n","        self.conv2 = nn.Conv1d(32, 32, kernel_size=1)\n","        self.flatten = nn.Flatten()\n","\n","        self.layers = nn.ModuleList()\n","        self.layers.append(nn.Linear(input_dim * 32, neurons))\n","        self.layers.append(getattr(nn, activation)())\n","        for _ in range(layers1):\n","            self.layers.append(nn.Linear(neurons, neurons))\n","            self.layers.append(getattr(nn, activation)())\n","            if dropout_rate > 0:\n","                self.layers.append(nn.Dropout(p=dropout_rate))\n","\n","        self.output_layer = nn.Linear(neurons, 1)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, 1, seq_length)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.flatten(x)  # shape: (batch_size, seq_length * 32)\n","        for layer in self.layers:\n","            x = layer(x)\n","        x = self.output_layer(x)\n","        return x\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# -----------------------------------------------------------------------\n","# 4) Hyper-params and DataLoader\n","# -----------------------------------------------------------------------\n","activation = 'ReLU'\n","batch_size = 64\n","dropout_rate = 0.05\n","epochs = 60\n","layers1 = 2\n","learning_rate = 0.001\n","neurons = 32\n","input_dim = train_X.shape[1]\n","\n","# Reshape for conv layers -> (batch, channels=1, seq_length)\n","train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n","test_X  = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n","\n","train_dataset = TensorDataset(train_X, train_y)\n","test_dataset  = TensorDataset(test_X, test_y)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","model = NeuralNetwork(input_dim, neurons, layers1, activation, dropout_rate).to(device)\n","print(f\"Trainable parameters: {count_parameters(model)}\")\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# -----------------------------------------------------------------------\n","# 5) Monitor GPU power & Train\n","# -----------------------------------------------------------------------\n","# Before starting training power monitoring\n","clear_log_file(training_power_log)\n","\n","\n","print(\"Starting training and logging power consumption...\")\n","train_power_log = \"training_power_log.csv\"\n","monitor_process_train = monitor_power(train_power_log)\n","\n","start_time = time.time()\n","best_val_loss = float('inf')\n","\n","for epoch in range(epochs):\n","    model.train()\n","    epoch_loss = 0.0\n","    for batch_X, batch_y in train_loader:\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(batch_X)\n","        loss = criterion(outputs, batch_y)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for val_X, val_y in test_loader:\n","            val_X, val_y = val_X.to(device), val_y.to(device)\n","            val_out = model(val_X)\n","            val_loss += criterion(val_out, val_y).item()\n","\n","    val_loss /= len(test_loader)\n","    avg_train_loss = epoch_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n","\n","    # Track best val loss\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","end_time = time.time()\n","monitor_process_train.terminate()\n","monitor_process_train.wait()\n","\n","# Calculate training energy usage\n","print(\"Training complete. Calculating power consumption...\")\n","train_energy_joules, train_energy_kwh = calculate_energy_consumption(train_power_log)\n","print(f\"Training Time: {end_time - start_time:.2f} sec\")\n","print(f\"Training Energy Consumption: {train_energy_joules:.2f} Joules ({train_energy_kwh:.6f} kWh)\")\n","\n","# -----------------------------------------------------------------------\n","# 6) Monitor GPU Power & Inference\n","# -----------------------------------------------------------------------\n","clear_log_file(inference_power_log)\n","\n","print(\"Starting inference and logging power consumption...\")\n","inference_power_log = \"inference_power_log.csv\"\n","monitor_process_infer = monitor_power(inference_power_log)\n","time.sleep(1)  # give nvidia-smi a moment to start logging\n","\n","model.eval()\n","with torch.no_grad():\n","    # Let’s artificially loop a few times to measure inference consumption\n","    for _ in range(1):\n","        preds = model(test_X.to(device))\n","\n","# Stop logging for inference\n","time.sleep(1)\n","monitor_process_infer.terminate()\n","monitor_process_infer.wait()\n","\n","inference_energy_joules, inference_energy_kwh = calculate_energy_consumption(inference_power_log)\n","print(f\"Inference Energy Consumption: {inference_energy_joules:.2f} Joules ({inference_energy_kwh:.6f} kWh)\")\n","\n","# -----------------------------------------------------------------------\n","# 7) Evaluate Results (Unnormalized)\n","# -----------------------------------------------------------------------\n","predictions = preds.cpu().numpy()\n","predictions_unnorm = scaler.inverse_transform(predictions)\n","test_y_unnorm = scaler.inverse_transform(test_y.numpy())\n","\n","mse_unnorm = mean_squared_error(test_y_unnorm, predictions_unnorm)\n","rmse_unnorm = np.sqrt(mse_unnorm)\n","mae_unnorm = mean_absolute_error(test_y_unnorm, predictions_unnorm)\n","\n","print(\"Final Unnormalized Metrics (on last inference results):\")\n","print(f\" MSE:  {mse_unnorm:.6f}\")\n","print(f\" RMSE: {rmse_unnorm:.6f}\")\n","print(f\" MAE:  {mae_unnorm:.6f}\")\n","print(f\"Best Validation Loss (MSE): {best_val_loss:.6f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"5e641005-ff8e-4664-9353-3c80f1b1f120","metadata":{"id":"5e641005-ff8e-4664-9353-3c80f1b1f120","outputId":"e3161bb2-3f81-43de-b82e-a680e9b4023d"},"outputs":[{"name":"stdout","output_type":"stream","text":["NeuralNetwork(\n","  (conv1): Conv1d(1, 32, kernel_size=(1,), stride=(1,))\n","  (conv2): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (layers): ModuleList(\n","    (0): Linear(in_features=576, out_features=100, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=100, out_features=100, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=100, out_features=100, bias=True)\n","    (5): ReLU()\n","  )\n","  (output_layer): Linear(in_features=100, out_features=1, bias=True)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"markdown","id":"3efc2438-75b0-4caf-923d-185e5156ae7f","metadata":{"id":"3efc2438-75b0-4caf-923d-185e5156ae7f"},"source":["# LSTM"]},{"cell_type":"code","execution_count":8,"id":"c86810d0-08e5-47e6-85c2-9fee5953305d","metadata":{"id":"c86810d0-08e5-47e6-85c2-9fee5953305d","outputId":"4d39abe7-64da-4edb-f791-ad74e33be624","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736800363407,"user_tz":-120,"elapsed":82157,"user":{"displayName":"bahgat ayasi","userId":"01883633993023881350"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-653145e9ab7b>:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  data = read_csv('jordan_pv.csv', parse_dates=['date'], index_col='date')\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Trainable parameters: 15329\n","Cleared existing log file: training_power_log.csv\n","Starting training and logging power consumption...\n","Epoch 1/60, Train Loss: 0.037486, Val Loss: 0.001621\n","Epoch 2/60, Train Loss: 0.004545, Val Loss: 0.001941\n","Epoch 3/60, Train Loss: 0.004216, Val Loss: 0.001443\n","Epoch 4/60, Train Loss: 0.003885, Val Loss: 0.001006\n","Epoch 5/60, Train Loss: 0.003747, Val Loss: 0.000736\n","Epoch 6/60, Train Loss: 0.003550, Val Loss: 0.000998\n","Epoch 7/60, Train Loss: 0.003496, Val Loss: 0.000765\n","Epoch 8/60, Train Loss: 0.003333, Val Loss: 0.000742\n","Epoch 9/60, Train Loss: 0.003311, Val Loss: 0.000663\n","Epoch 10/60, Train Loss: 0.003212, Val Loss: 0.000675\n","Epoch 11/60, Train Loss: 0.003246, Val Loss: 0.000983\n","Epoch 12/60, Train Loss: 0.003178, Val Loss: 0.001312\n","Epoch 13/60, Train Loss: 0.003205, Val Loss: 0.000820\n","Epoch 14/60, Train Loss: 0.003098, Val Loss: 0.000999\n","Epoch 15/60, Train Loss: 0.003135, Val Loss: 0.000628\n","Epoch 16/60, Train Loss: 0.003066, Val Loss: 0.001756\n","Epoch 17/60, Train Loss: 0.003096, Val Loss: 0.002633\n","Epoch 18/60, Train Loss: 0.003034, Val Loss: 0.000551\n","Epoch 19/60, Train Loss: 0.003117, Val Loss: 0.001157\n","Epoch 20/60, Train Loss: 0.003111, Val Loss: 0.000821\n","Epoch 21/60, Train Loss: 0.003080, Val Loss: 0.001144\n","Epoch 22/60, Train Loss: 0.003035, Val Loss: 0.000928\n","Epoch 23/60, Train Loss: 0.003114, Val Loss: 0.000570\n","Epoch 24/60, Train Loss: 0.003106, Val Loss: 0.000580\n","Epoch 25/60, Train Loss: 0.003004, Val Loss: 0.000633\n","Epoch 26/60, Train Loss: 0.003030, Val Loss: 0.001137\n","Epoch 27/60, Train Loss: 0.003036, Val Loss: 0.000967\n","Epoch 28/60, Train Loss: 0.003088, Val Loss: 0.000620\n","Epoch 29/60, Train Loss: 0.003038, Val Loss: 0.001199\n","Epoch 30/60, Train Loss: 0.003046, Val Loss: 0.000842\n","Epoch 31/60, Train Loss: 0.003086, Val Loss: 0.000738\n","Epoch 32/60, Train Loss: 0.003031, Val Loss: 0.000558\n","Epoch 33/60, Train Loss: 0.003053, Val Loss: 0.000614\n","Epoch 34/60, Train Loss: 0.003066, Val Loss: 0.000878\n","Epoch 35/60, Train Loss: 0.003007, Val Loss: 0.001417\n","Epoch 36/60, Train Loss: 0.003038, Val Loss: 0.000689\n","Epoch 37/60, Train Loss: 0.003017, Val Loss: 0.001022\n","Epoch 38/60, Train Loss: 0.003015, Val Loss: 0.000856\n","Epoch 39/60, Train Loss: 0.003043, Val Loss: 0.000835\n","Epoch 40/60, Train Loss: 0.003029, Val Loss: 0.001045\n","Epoch 41/60, Train Loss: 0.003007, Val Loss: 0.001320\n","Epoch 42/60, Train Loss: 0.003057, Val Loss: 0.001358\n","Epoch 43/60, Train Loss: 0.003011, Val Loss: 0.000899\n","Epoch 44/60, Train Loss: 0.003005, Val Loss: 0.000694\n","Epoch 45/60, Train Loss: 0.003114, Val Loss: 0.001303\n","Epoch 46/60, Train Loss: 0.002995, Val Loss: 0.000512\n","Epoch 47/60, Train Loss: 0.002942, Val Loss: 0.000870\n","Epoch 48/60, Train Loss: 0.003033, Val Loss: 0.001716\n","Epoch 49/60, Train Loss: 0.002969, Val Loss: 0.000937\n","Epoch 50/60, Train Loss: 0.002972, Val Loss: 0.000723\n","Epoch 51/60, Train Loss: 0.002957, Val Loss: 0.001067\n","Epoch 52/60, Train Loss: 0.002993, Val Loss: 0.000563\n","Epoch 53/60, Train Loss: 0.003037, Val Loss: 0.001836\n","Epoch 54/60, Train Loss: 0.003082, Val Loss: 0.001242\n","Epoch 55/60, Train Loss: 0.003020, Val Loss: 0.001129\n","Epoch 56/60, Train Loss: 0.003008, Val Loss: 0.001189\n","Epoch 57/60, Train Loss: 0.002993, Val Loss: 0.001618\n","Epoch 58/60, Train Loss: 0.003025, Val Loss: 0.001143\n","Epoch 59/60, Train Loss: 0.002967, Val Loss: 0.000810\n","Epoch 60/60, Train Loss: 0.003017, Val Loss: 0.001162\n","Training completed!\n","Training Time: 76.99 sec\n","Training Energy Consumption: 2502.28 Joules (0.000695 kWh)\n","Cleared existing log file: inference_power_log.csv\n","Starting inference and logging power consumption...\n","Inference Completed!\n","Inference Energy Consumption: 64.68 Joules (0.000018 kWh)\n","\n","Final Unnormalized Metrics (on last inference results):\n"," MSE:  1384.666870\n"," RMSE: 37.211112\n"," MAE:  25.561050\n","Best Validation Loss (MSE): 0.000512\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import pandas as pd\n","import time\n","import subprocess\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from pandas import read_csv\n","import os\n","# ---------------------------------------------------------\n","# 1) GPU power monitoring functions\n","# ---------------------------------------------------------\n","def monitor_power(output_file):\n","    \"\"\"\n","    Launch nvidia-smi in the background, logging instantaneous GPU power usage (W) every 1 second.\n","    \"\"\"\n","    command = f\"nvidia-smi --query-gpu=power.draw --format=csv -l 1 > {output_file}\"\n","    process = subprocess.Popen(command, shell=True)\n","    return process\n","\n","def calculate_energy_consumption(log_file):\n","    \"\"\"\n","    Read the CSV log with GPU power (in W) every second.\n","    Sum them up to get total Joules. Then convert Joules -> kWh.\n","    \"\"\"\n","    data = pd.read_csv(log_file, skiprows=1, names=[\"Power (W)\"])\n","    # Clean up any trailing \" W\" or similar\n","    data[\"Power (W)\"] = data[\"Power (W)\"].str.extract(r'(\\d+\\.\\d+|\\d+)').astype(float)\n","    # Energy(J) = sum of (Power_in_W * 1 second)\n","    energy_joules = data[\"Power (W)\"].sum()\n","    # Convert to kWh\n","    energy_kwh = energy_joules / (3600 * 1000)\n","    return energy_joules, energy_kwh\n","\n","# Add this at the beginning of your script or before starting power monitoring\n","def clear_log_file(log_file):\n","    if os.path.exists(log_file):\n","        os.remove(log_file)\n","        print(f\"Cleared existing log file: {log_file}\")\n","\n","# ---------------------------------------------------------\n","# 2) Load & preprocess data\n","# ---------------------------------------------------------\n","data = read_csv('jordan_pv.csv', parse_dates=['date'], index_col='date')\n","#data = read_csv('Palestine-PV.csv', parse_dates=['date'], index_col='date')\n","\n","data.index = pd.to_datetime(data.index)\n","data = data.sort_index()\n","\n","# Select column 6 (index 5), drop NaN\n","data = data.iloc[:, 5].dropna().astype('float32')\n","\n","# Convert to numpy and scale\n","values = np.array(data).reshape(-1, 1)\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled_data = scaler.fit_transform(values)\n","\n","# Train/test split\n","train_size = int(len(scaled_data) * 0.8)\n","train_data = scaled_data[:train_size]\n","test_data = scaled_data[train_size:]\n","\n","def to_sequences(dataset, seq_len=1):\n","    x, y = [], []\n","    for i in range(len(dataset) - seq_len - 1):\n","        x.append(dataset[i:(i + seq_len), 0])\n","        y.append(dataset[i + seq_len, 0])\n","    return np.array(x), np.array(y)\n","\n","seq_size = 3\n","train_X, train_y = to_sequences(train_data, seq_size)\n","test_X, test_y = to_sequences(test_data, seq_size)\n","\n","# Convert to Torch tensor\n","train_X = torch.tensor(train_X).float()\n","train_y = torch.tensor(train_y).view(-1,1).float()\n","test_X  = torch.tensor(test_X).float()\n","test_y  = torch.tensor(test_y).view(-1,1).float()\n","\n","# ---------------------------------------------------------\n","# 3) Define the LSTM-based model\n","# ---------------------------------------------------------\n","class LSTMNetwork(nn.Module):\n","    def __init__(self, input_dim, neurons, layers1, activation, dropout_rate):\n","        super(LSTMNetwork, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, neurons, layers1, batch_first=True)\n","        self.layers = nn.ModuleList()\n","        self.layers.append(nn.Linear(neurons, neurons))\n","        self.layers.append(getattr(nn, activation)())\n","        self.layers.append(nn.Dropout(p=dropout_rate))  # Dropout after first dense layer\n","        for _ in range(layers1 - 1):\n","            self.layers.append(nn.Linear(neurons, neurons))\n","            self.layers.append(getattr(nn, activation)())\n","            if dropout_rate > 0:\n","                self.layers.append(nn.Dropout(p=dropout_rate))  # Dropout after each dense layer\n","        self.output_layer = nn.Linear(neurons, 1)\n","\n","    def forward(self, x):\n","        x, _ = self.lstm(x)\n","        x = x[:, -1, :]  # get the last output of the sequence\n","        for layer in self.layers:\n","            x = layer(x)\n","        x = self.output_layer(x)\n","        return x\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# ---------------------------------------------------------\n","# 4) Hyperparams, DataLoaders, & Setup\n","# ---------------------------------------------------------\n","activation='ReLU'\n","neurons=32\n","layers1=2\n","dropout_rate=0.05\n","epochs=60\n","learning_rate=0.001\n","batch_size=64\n","\n","input_dim = train_X.shape[1]  # seq_size\n","# Reshape for conv -> shape: (N, channels=1, seq_len)\n","train_X = train_X.reshape(train_X.shape[0], 1, train_X.shape[1])\n","test_X  = test_X.reshape(test_X.shape[0], 1, test_X.shape[1])\n","\n","train_dataset = TensorDataset(train_X, train_y)\n","test_dataset  = TensorDataset(test_X, test_y)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","model = LSTMNetwork(input_dim, neurons, layers1, activation, dropout_rate).to(device)\n","nparams = count_parameters(model)\n","print(f\"Trainable parameters: {nparams}\")\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# ---------------------------------------------------------\n","# 5) TRAINING - Monitor GPU power\n","# ---------------------------------------------------------\n","# Before starting training power monitoring\n","clear_log_file(training_power_log)\n","train_power_log = \"training_power_log.csv\"\n","print(\"Starting training and logging power consumption...\")\n","monitor_proc = monitor_power(train_power_log)\n","start_time = time.time()\n","\n","best_val_loss = float('inf')\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for batchX, batchY in train_loader:\n","        batchX, batchY = batchX.to(device), batchY.to(device)\n","        optimizer.zero_grad()\n","        out = model(batchX)\n","        loss = criterion(out, batchY)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    train_loss_epoch = running_loss / len(train_loader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for valX, valY in test_loader:\n","            valX, valY = valX.to(device), valY.to(device)\n","            predY = model(valX)\n","            val_loss += criterion(predY, valY).item()\n","    val_loss /= len(test_loader)\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss_epoch:.6f}, Val Loss: {val_loss:.6f}\")\n","    # Track best val\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), \"best_model.pth\")\n","\n","end_time = time.time()\n","monitor_proc.terminate()\n","monitor_proc.wait()\n","\n","train_energy_joules, train_energy_kwh = calculate_energy_consumption(train_power_log)\n","train_time = end_time - start_time\n","print(\"Training completed!\")\n","print(f\"Training Time: {train_time:.2f} sec\")\n","print(f\"Training Energy Consumption: {train_energy_joules:.2f} Joules ({train_energy_kwh:.6f} kWh)\")\n","\n","# ---------------------------------------------------------\n","# 6) INFERENCE - Monitor GPU power\n","# ---------------------------------------------------------\n","# Before starting inference power monitoring\n","clear_log_file(inference_power_log)\n","\n","inference_power_log = \"inference_power_log.csv\"\n","print(\"Starting inference and logging power consumption...\")\n","monitor_proc_infer = monitor_power(inference_power_log)\n","time.sleep(1)  # give nvidia-smi time to start logging\n","\n","model.eval()\n","with torch.no_grad():\n","    # artificially enlarge the inference workload\n","    for _ in range(1):\n","        predictions = model(test_X.to(device))\n","\n","time.sleep(1)  # ensure last logs are written\n","monitor_proc_infer.terminate()\n","monitor_proc_infer.wait()\n","\n","inference_energy_joules, inference_energy_kwh = calculate_energy_consumption(inference_power_log)\n","print(\"Inference Completed!\")\n","print(f\"Inference Energy Consumption: {inference_energy_joules:.2f} Joules ({inference_energy_kwh:.6f} kWh)\")\n","\n","# ---------------------------------------------------------\n","# 7) Print out all final metrics, exactly as in the original script\n","# ---------------------------------------------------------\n","predictions_np = predictions.cpu().numpy()\n","test_y_np = test_y.cpu().numpy()\n","\n","# Inverse transform\n","pred_unnorm = scaler.inverse_transform(predictions_np)\n","test_y_unnorm = scaler.inverse_transform(test_y_np)\n","\n","# Compute MSE, RMSE, MAE on unnormalized data\n","mse_unnorm = mean_squared_error(test_y_unnorm, pred_unnorm)\n","rmse_unnorm = np.sqrt(mse_unnorm)\n","mae_unnorm = mean_absolute_error(test_y_unnorm, pred_unnorm)\n","\n","print(\"\\nFinal Unnormalized Metrics (on last inference results):\")\n","print(f\" MSE:  {mse_unnorm:.6f}\")\n","print(f\" RMSE: {rmse_unnorm:.6f}\")\n","print(f\" MAE:  {mae_unnorm:.6f}\")\n","print(f\"Best Validation Loss (MSE): {best_val_loss:.6f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"f9dc567d-26cc-4d27-af6e-59fcca646ebf","metadata":{"id":"f9dc567d-26cc-4d27-af6e-59fcca646ebf","outputId":"f9225223-abc0-492a-baf9-d5bd5605bb7a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736630469964,"user_tz":-120,"elapsed":315,"user":{"displayName":"bahgat ayasi","userId":"01883633993023881350"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LSTMNetwork(\n","  (lstm): LSTM(3, 32, num_layers=2, batch_first=True)\n","  (layers): ModuleList(\n","    (0): Linear(in_features=32, out_features=32, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.05, inplace=False)\n","    (3): Linear(in_features=32, out_features=32, bias=True)\n","    (4): ReLU()\n","    (5): Dropout(p=0.05, inplace=False)\n","  )\n","  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":null,"id":"56da4621-983c-41ae-95f4-5ccca9ab6271","metadata":{"id":"56da4621-983c-41ae-95f4-5ccca9ab6271"},"outputs":[],"source":["# MLP Mutivarite"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","import subprocess\n","import time\n","\n","# Function to monitor power usage with nvidia-smi\n","def monitor_power(output_file):\n","    # Run nvidia-smi in the background to log power usage\n","    command = f\"nvidia-smi --query-gpu=power.draw --format=csv -l 1 > {output_file}\"\n","    process = subprocess.Popen(command, shell=True)\n","    return process\n","\n","# Function to calculate energy consumption from the log\n","def calculate_energy_consumption(log_file):\n","    # Load the power log file\n","    data = pd.read_csv(log_file, skiprows=1, names=[\"Power (W)\"])\n","\n","    # Debug: Check log content\n","    print(f\"Log file content:\\n{data.head()}\")\n","\n","    # Clean the \"Power (W)\" column to remove non-numeric characters (e.g., \" W\")\n","    data[\"Power (W)\"] = data[\"Power (W)\"].str.extract(r'(\\d+\\.\\d+|\\d+)').astype(float)\n","\n","    # Debug: Check cleaned data\n","    print(f\"Cleaned log data:\\n{data.head()}\")\n","\n","    # Calculate total energy in Joules (assuming logs are every 1 second)\n","    energy_joules = data[\"Power (W)\"].sum()  # Energy = Power × Time (1 second per log)\n","\n","    # Convert total energy to kWh\n","    energy_kwh = energy_joules / (3600 * 1000)\n","\n","    return energy_joules, energy_kwh\n","# Add this at the beginning of your script or before starting power monitoring\n","def clear_log_file(log_file):\n","    if os.path.exists(log_file):\n","        os.remove(log_file)\n","        print(f\"Cleared existing log file: {log_file}\")\n","\n","# Load and preprocess the dataset\n","#dataset = pd.read_csv('Palestine-PV.csv', parse_dates=['date'], index_col='date')\n","dataset = pd.read_csv('jordan_pv.csv',  index_col='date')\n","dataset.index = pd.to_datetime(dataset.index)\n","dataset = dataset.sort_index()\n","dataset = dataset.dropna()\n","dataset = dataset.astype(np.float64)\n","\n","def series_to_supervised(data, target_col_name, n_in=1, n_out=1, dropnan=True):\n","    n_vars = data.shape[1]\n","    df = pd.DataFrame(data)\n","    cols, names = list(), list()\n","    df = df[[col for col in df if col != target_col_name] + [target_col_name]]\n","    for i in range(n_in, 0, -1):\n","        cols.append(df.shift(i))\n","        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","    for i in range(0, n_out):\n","        cols.append(df.shift(-i))\n","        if i == 0:\n","            names += [('var%d(t)' % (j+1)) for j in range(n_vars-1)]\n","            names.append(target_col_name + '(t)')\n","        else:\n","            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","    agg = pd.concat(cols, axis=1)\n","    agg.columns = names\n","    agg = agg[[col for col in agg if target_col_name in col or '(t)' not in col]]\n","    if dropnan:\n","        agg.dropna(inplace=True)\n","    return agg\n","\n","values = dataset.values\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled = scaler.fit_transform(values)\n","df_scaled = pd.DataFrame(scaled, columns=dataset.columns)\n","target_scaler = MinMaxScaler(feature_range=(0, 1))\n","target_scaled = target_scaler.fit_transform(dataset[['Solar Radiation(GHI)']])\n","reframed = series_to_supervised(df_scaled, 'Solar Radiation(GHI)', 3, 1)\n","values = reframed.values\n","n_train_hours = 876 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","train_X, train_y = train[:, :-1], train[:, -1]\n","test_X, test_y = test[:, :-1], test[:, -1]\n","train_X, train_y = torch.Tensor(train_X), torch.Tensor(train_y).view(-1, 1)\n","test_X, test_y = torch.Tensor(test_X), torch.Tensor(test_y).view(-1, 1)\n","\n","\n","# Define the model\n","class NeuralNetwork(nn.Module):\n","    def __init__(self, input_dim, neurons, layers1, activation, dropout_rate):\n","        super(NeuralNetwork, self).__init__()\n","        self.layers = nn.ModuleList()\n","        self.layers.append(nn.Linear(input_dim, neurons))\n","        self.layers.append(getattr(nn, activation)())\n","        for _ in range(layers1):\n","            self.layers.append(nn.Linear(neurons, neurons))\n","            self.layers.append(getattr(nn, activation)())\n","            if dropout_rate > 0:\n","                self.layers.append(nn.Dropout(p=dropout_rate))\n","        self.layers.append(nn.Linear(neurons, 1))\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","# Set up the device (GPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# Initialize the model, loss function, and optimizer\n","activation = 'ReLU'\n","neurons = 32\n","layers1 = 2\n","dropout_rate = 0.05\n","input_dim = train_X.shape[1]\n","learning_rate = 0.001\n","\n","model = NeuralNetwork(input_dim, neurons, layers1, activation, dropout_rate).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Prepare the data loaders\n","batch_size = 64\n","train_dataset = TensorDataset(train_X, train_y)\n","test_dataset = TensorDataset(test_X, test_y)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Train the model and monitor power consumption\n","epochs = 60\n","train_losses, val_losses = [], []\n","min_val_loss = float('inf')\n","\n","# Before starting training power monitoring\n","clear_log_file(training_power_log)\n","\n","print(\"Starting training and logging power consumption...\")\n","training_power_log = \"training_power_log.csv\"\n","monitor_process = monitor_power(training_power_log)\n","\n","# ---- Add training time measurement ----\n","start_time = time.time()\n","\n","for epoch in range(epochs):\n","    model.train()\n","    epoch_loss = 0\n","    for batch_X, batch_y in train_loader:\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(batch_X)\n","        loss = criterion(outputs, batch_y)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for val_X, val_y in test_loader:\n","            val_X, val_y = val_X.to(device), val_y.to(device)\n","            val_outputs = model(val_X)\n","            val_loss += criterion(val_outputs, val_y).item()\n","    val_loss /= len(test_loader)\n","    train_losses.append(epoch_loss / len(train_loader))\n","    val_losses.append(val_loss)\n","    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {epoch_loss / len(train_loader):.6f}, Val Loss: {val_loss:.6f}\")\n","\n","    if val_loss < min_val_loss:\n","        min_val_loss = val_loss\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","# Stop the power monitoring for training\n","end_time = time.time()\n","monitor_process.terminate()\n","monitor_process.wait()\n","\n","training_time = end_time - start_time\n","print(f\"Training complete. Training time: {training_time:.2f} seconds.\")\n","print(\"Calculating power consumption for training...\")\n","\n","# Calculate energy consumption for training\n","train_energy_joules, train_energy_kwh = calculate_energy_consumption(training_power_log)\n","print(f\"Training Energy Consumption: {train_energy_joules:.2f} Joules ({train_energy_kwh:.6f} kWh)\")\n","\n","# Before starting inference power monitoring\n","clear_log_file(inference_power_log)\n","\n","# Inference and monitor power consumption\n","inference_power_log = \"inference_power_log.csv\"\n","print(\"Starting inference and logging power consumption...\")\n","monitor_process = monitor_power(inference_power_log)\n","\n","# Add delay to ensure logging starts\n","time.sleep(1)\n","\n","model.eval()\n","with torch.no_grad():\n","    for _ in range(1):  # Artificially increase inference workload\n","        predictions = model(test_X.to(device)).cpu().numpy()\n","\n","# Add delay to ensure all logs are written\n","time.sleep(1)\n","\n","# Stop the power monitoring for inference\n","monitor_process.terminate()\n","monitor_process.wait()\n","print(\"Inference complete. Calculating power consumption...\")\n","\n","# Calculate energy consumption for inference\n","inference_energy_joules, inference_energy_kwh = calculate_energy_consumption(inference_power_log)\n","print(f\"Inference Energy Consumption: {inference_energy_joules:.2f} Joules ({inference_energy_kwh:.6f} kWh)\")\n","\n","# Rescale predictions and evaluate\n","predictions_unnorm = scaler.inverse_transform(predictions)\n","test_y_unnorm = scaler.inverse_transform(test_y.numpy())\n","\n","mse = mean_squared_error(test_y_unnorm, predictions_unnorm)\n","rmse = np.sqrt(mse)\n","mae = mean_absolute_error(test_y_unnorm, predictions_unnorm)\n","\n","print(f\"Unnormalized MSE: {mse:.6f}\")\n","print(f\"Unnormalized RMSE: {rmse:.6f}\")\n","print(f\"Unnormalized MAE: {mae:.6f}\")\n"],"metadata":{"id":"jyyut0PNnu30","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1736800672683,"user_tz":-120,"elapsed":56636,"user":{"displayName":"bahgat ayasi","userId":"01883633993023881350"}},"outputId":"4b1ab084-32f6-4c28-b2b8-b820c159bc55"},"id":"jyyut0PNnu30","execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-5d0b188e2c77>:49: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  dataset.index = pd.to_datetime(dataset.index)\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Cleared existing log file: training_power_log.csv\n","Starting training and logging power consumption...\n","Epoch 1/60, Train Loss: 0.020542, Val Loss: 0.001442\n","Epoch 2/60, Train Loss: 0.004146, Val Loss: 0.000958\n","Epoch 3/60, Train Loss: 0.003682, Val Loss: 0.000825\n","Epoch 4/60, Train Loss: 0.003380, Val Loss: 0.000682\n","Epoch 5/60, Train Loss: 0.003201, Val Loss: 0.000817\n","Epoch 6/60, Train Loss: 0.003074, Val Loss: 0.000633\n","Epoch 7/60, Train Loss: 0.003015, Val Loss: 0.000820\n","Epoch 8/60, Train Loss: 0.002894, Val Loss: 0.000471\n","Epoch 9/60, Train Loss: 0.002968, Val Loss: 0.000624\n","Epoch 10/60, Train Loss: 0.002971, Val Loss: 0.000582\n","Epoch 11/60, Train Loss: 0.002847, Val Loss: 0.000529\n","Epoch 12/60, Train Loss: 0.002721, Val Loss: 0.000433\n","Epoch 13/60, Train Loss: 0.002746, Val Loss: 0.000783\n","Epoch 14/60, Train Loss: 0.002677, Val Loss: 0.000470\n","Epoch 15/60, Train Loss: 0.002690, Val Loss: 0.000484\n","Epoch 16/60, Train Loss: 0.002650, Val Loss: 0.000994\n","Epoch 17/60, Train Loss: 0.002614, Val Loss: 0.000599\n","Epoch 18/60, Train Loss: 0.002636, Val Loss: 0.000487\n","Epoch 19/60, Train Loss: 0.002637, Val Loss: 0.000455\n","Epoch 20/60, Train Loss: 0.002575, Val Loss: 0.000459\n","Epoch 21/60, Train Loss: 0.002579, Val Loss: 0.000421\n","Epoch 22/60, Train Loss: 0.002583, Val Loss: 0.000636\n","Epoch 23/60, Train Loss: 0.002544, Val Loss: 0.000502\n","Epoch 24/60, Train Loss: 0.002524, Val Loss: 0.000591\n","Epoch 25/60, Train Loss: 0.002564, Val Loss: 0.000626\n","Epoch 26/60, Train Loss: 0.002482, Val Loss: 0.000389\n","Epoch 27/60, Train Loss: 0.002495, Val Loss: 0.000601\n","Epoch 28/60, Train Loss: 0.002464, Val Loss: 0.000691\n","Epoch 29/60, Train Loss: 0.002536, Val Loss: 0.000355\n","Epoch 30/60, Train Loss: 0.002495, Val Loss: 0.000482\n","Epoch 31/60, Train Loss: 0.002439, Val Loss: 0.000999\n","Epoch 32/60, Train Loss: 0.002421, Val Loss: 0.000972\n","Epoch 33/60, Train Loss: 0.002434, Val Loss: 0.000761\n","Epoch 34/60, Train Loss: 0.002430, Val Loss: 0.000401\n","Epoch 35/60, Train Loss: 0.002455, Val Loss: 0.000705\n","Epoch 36/60, Train Loss: 0.002443, Val Loss: 0.000476\n","Epoch 37/60, Train Loss: 0.002377, Val Loss: 0.000808\n","Epoch 38/60, Train Loss: 0.002395, Val Loss: 0.000849\n","Epoch 39/60, Train Loss: 0.002472, Val Loss: 0.000653\n","Epoch 40/60, Train Loss: 0.002324, Val Loss: 0.000457\n","Epoch 41/60, Train Loss: 0.002411, Val Loss: 0.000474\n","Epoch 42/60, Train Loss: 0.002350, Val Loss: 0.001273\n","Epoch 43/60, Train Loss: 0.002345, Val Loss: 0.000989\n","Epoch 44/60, Train Loss: 0.002345, Val Loss: 0.000552\n","Epoch 45/60, Train Loss: 0.002342, Val Loss: 0.000604\n","Epoch 46/60, Train Loss: 0.002370, Val Loss: 0.000363\n","Epoch 47/60, Train Loss: 0.002344, Val Loss: 0.000871\n","Epoch 48/60, Train Loss: 0.002319, Val Loss: 0.000768\n","Epoch 49/60, Train Loss: 0.002319, Val Loss: 0.000363\n","Epoch 50/60, Train Loss: 0.002310, Val Loss: 0.000601\n","Epoch 51/60, Train Loss: 0.002322, Val Loss: 0.000598\n","Epoch 52/60, Train Loss: 0.002258, Val Loss: 0.000605\n","Epoch 53/60, Train Loss: 0.002330, Val Loss: 0.000723\n","Epoch 54/60, Train Loss: 0.002239, Val Loss: 0.000742\n","Epoch 55/60, Train Loss: 0.002293, Val Loss: 0.000465\n","Epoch 56/60, Train Loss: 0.002253, Val Loss: 0.000869\n","Epoch 57/60, Train Loss: 0.002218, Val Loss: 0.000759\n","Epoch 58/60, Train Loss: 0.002270, Val Loss: 0.000397\n","Epoch 59/60, Train Loss: 0.002262, Val Loss: 0.000467\n","Epoch 60/60, Train Loss: 0.002261, Val Loss: 0.000883\n","Training complete. Training time: 52.28 seconds.\n","Calculating power consumption for training...\n","Log file content:\n","  Power (W)\n","0   32.14 W\n","1   32.44 W\n","2   32.34 W\n","3   32.34 W\n","4   32.34 W\n","Cleaned log data:\n","   Power (W)\n","0      32.14\n","1      32.44\n","2      32.34\n","3      32.34\n","4      32.34\n","Training Energy Consumption: 1712.34 Joules (0.000476 kWh)\n","Cleared existing log file: inference_power_log.csv\n","Starting inference and logging power consumption...\n","Inference complete. Calculating power consumption...\n","Log file content:\n","  Power (W)\n","0   32.24 W\n","1   32.14 W\n","Cleaned log data:\n","   Power (W)\n","0      32.24\n","1      32.14\n","Inference Energy Consumption: 64.38 Joules (0.000018 kWh)\n"]},{"output_type":"error","ename":"ValueError","evalue":"non-broadcastable output operand with shape (5253,1) doesn't match the broadcast shape (5253,6)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-5d0b188e2c77>\u001b[0m in \u001b[0;36m<cell line: 221>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;31m# Rescale predictions and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m \u001b[0mpredictions_unnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0mtest_y_unnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    579\u001b[0m         )\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (5253,1) doesn't match the broadcast shape (5253,6)"]}]},{"cell_type":"code","source":["# CNN Multivariate"],"metadata":{"id":"HAVN25CykQYN"},"id":"HAVN25CykQYN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import time\n","from sklearn.preprocessing import MinMaxScaler\n","from pandas import read_csv\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","import subprocess\n","\n","# -----------------------------------------------------------------------\n","# 1) Functions for GPU Power Monitoring, as in the second script\n","# -----------------------------------------------------------------------\n","def monitor_power(output_file):\n","    \"\"\"\n","    Run nvidia-smi in the background to log power usage (in W) every 1 second.\n","    \"\"\"\n","    command = f\"nvidia-smi --query-gpu=power.draw --format=csv -l 1 > {output_file}\"\n","    process = subprocess.Popen(command, shell=True)\n","    return process\n","\n","def calculate_energy_consumption(log_file):\n","    \"\"\"\n","    Parse the CSV log with GPU power usage and calculate:\n","      - total energy in Joules\n","      - total energy in kWh\n","    \"\"\"\n","    data = pd.read_csv(log_file, skiprows=1, names=[\"Power (W)\"])\n","\n","    # Clean the \"Power (W)\" column in case it contains the units\n","    data[\"Power (W)\"] = data[\"Power (W)\"].str.extract(r'(\\d+\\.\\d+|\\d+)').astype(float)\n","\n","    # Each reading is 1 second apart, so total Joules is sum(power_in_watts) * 1 second\n","    energy_joules = data[\"Power (W)\"].sum()\n","\n","    # Convert Joules to kWh\n","    energy_kwh = energy_joules / (3600 * 1000)\n","\n","    return energy_joules, energy_kwh\n","\n","# Add this at the beginning of your script or before starting power monitoring\n","def clear_log_file(log_file):\n","    if os.path.exists(log_file):\n","        os.remove(log_file)\n","        print(f\"Cleared existing log file: {log_file}\")\n","\n","# Load and preprocess the dataset\n","#dataset = pd.read_csv('Palestine-PV.csv', parse_dates=['date'], index_col='date')\n","dataset = pd.read_csv('jordan_pv.csv',  index_col='date')\n","dataset.index = pd.to_datetime(dataset.index)\n","dataset = dataset.sort_index()\n","dataset = dataset.dropna()\n","dataset = dataset.astype(np.float64)\n","\n","def series_to_supervised(data, target_col_name, n_in=1, n_out=1, dropnan=True):\n","    n_vars = data.shape[1]\n","    df = pd.DataFrame(data)\n","    cols, names = list(), list()\n","    df = df[[col for col in df if col != target_col_name] + [target_col_name]]\n","    for i in range(n_in, 0, -1):\n","        cols.append(df.shift(i))\n","        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","    for i in range(0, n_out):\n","        cols.append(df.shift(-i))\n","        if i == 0:\n","            names += [('var%d(t)' % (j+1)) for j in range(n_vars-1)]\n","            names.append(target_col_name + '(t)')\n","        else:\n","            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","    agg = pd.concat(cols, axis=1)\n","    agg.columns = names\n","    agg = agg[[col for col in agg if target_col_name in col or '(t)' not in col]]\n","    if dropnan:\n","        agg.dropna(inplace=True)\n","    return agg\n","\n","values = dataset.values\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled = scaler.fit_transform(values)\n","df_scaled = pd.DataFrame(scaled, columns=dataset.columns)\n","target_scaler = MinMaxScaler(feature_range=(0, 1))\n","target_scaled = target_scaler.fit_transform(dataset[['Solar Radiation(GHI)']])\n","reframed = series_to_supervised(df_scaled, 'Solar Radiation(GHI)', 3, 1)\n","values = reframed.values\n","n_train_hours = 876 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","train_X, train_y = train[:, :-1], train[:, -1]\n","test_X, test_y = test[:, :-1], test[:, -1]\n","train_X, train_y = torch.Tensor(train_X), torch.Tensor(train_y).view(-1, 1)\n","test_X, test_y = torch.Tensor(test_X), torch.Tensor(test_y).view(-1, 1)\n","# -----------------------------------------------------------------------\n","# 3) Define the Model\n","# -----------------------------------------------------------------------\n","class NeuralNetwork(nn.Module):\n","    def __init__(self, input_dim, neurons, layers1, activation, dropout_rate):\n","        super(NeuralNetwork, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 32, kernel_size=1)\n","        self.conv2 = nn.Conv1d(32, 32, kernel_size=1)\n","        self.flatten = nn.Flatten()\n","\n","        self.layers = nn.ModuleList()\n","        self.layers.append(nn.Linear(input_dim * 32, neurons))\n","        self.layers.append(getattr(nn, activation)())\n","        for _ in range(layers1):\n","            self.layers.append(nn.Linear(neurons, neurons))\n","            self.layers.append(getattr(nn, activation)())\n","            if dropout_rate > 0:\n","                self.layers.append(nn.Dropout(p=dropout_rate))\n","\n","        self.output_layer = nn.Linear(neurons, 1)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, 1, seq_length)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.flatten(x)  # shape: (batch_size, seq_length * 32)\n","        for layer in self.layers:\n","            x = layer(x)\n","        x = self.output_layer(x)\n","        return x\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# -----------------------------------------------------------------------\n","# 4) Hyper-params and DataLoader\n","# -----------------------------------------------------------------------\n","activation = 'ReLU'\n","batch_size = 64\n","dropout_rate = 0.05\n","epochs = 60\n","layers1 = 2\n","learning_rate = 0.001\n","neurons = 32\n","input_dim = train_X.shape[1]\n","\n","# Reshape for conv layers -> (batch, channels=1, seq_length)\n","train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n","test_X  = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n","\n","train_dataset = TensorDataset(train_X, train_y)\n","test_dataset  = TensorDataset(test_X, test_y)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","model = NeuralNetwork(input_dim, neurons, layers1, activation, dropout_rate).to(device)\n","print(f\"Trainable parameters: {count_parameters(model)}\")\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# -----------------------------------------------------------------------\n","# 5) Monitor GPU power & Train\n","# -----------------------------------------------------------------------\n","\n","# Before starting training power monitoring\n","clear_log_file(training_power_log)\n","\n","print(\"Starting training and logging power consumption...\")\n","train_power_log = \"training_power_log.csv\"\n","monitor_process_train = monitor_power(train_power_log)\n","\n","start_time = time.time()\n","best_val_loss = float('inf')\n","\n","for epoch in range(epochs):\n","    model.train()\n","    epoch_loss = 0.0\n","    for batch_X, batch_y in train_loader:\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(batch_X)\n","        loss = criterion(outputs, batch_y)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for val_X, val_y in test_loader:\n","            val_X, val_y = val_X.to(device), val_y.to(device)\n","            val_out = model(val_X)\n","            val_loss += criterion(val_out, val_y).item()\n","\n","    val_loss /= len(test_loader)\n","    avg_train_loss = epoch_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n","\n","    # Track best val loss\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","end_time = time.time()\n","monitor_process_train.terminate()\n","monitor_process_train.wait()\n","\n","# Calculate training energy usage\n","print(\"Training complete. Calculating power consumption...\")\n","train_energy_joules, train_energy_kwh = calculate_energy_consumption(train_power_log)\n","print(f\"Training Time: {end_time - start_time:.2f} sec\")\n","print(f\"Training Energy Consumption: {train_energy_joules:.2f} Joules ({train_energy_kwh:.6f} kWh)\")\n","\n","# -----------------------------------------------------------------------\n","# 6) Monitor GPU Power & Inference\n","# -----------------------------------------------------------------------\n","# Before starting inference power monitoring\n","clear_log_file(inference_power_log)\n","\n","print(\"Starting inference and logging power consumption...\")\n","inference_power_log = \"inference_power_log.csv\"\n","monitor_process_infer = monitor_power(inference_power_log)\n","time.sleep(1)  # give nvidia-smi a moment to start logging\n","\n","model.eval()\n","with torch.no_grad():\n","    # Let’s artificially loop a few times to measure inference consumption\n","    for _ in range(1):\n","        preds = model(test_X.to(device))\n","\n","# Stop logging for inference\n","time.sleep(1)\n","monitor_process_infer.terminate()\n","monitor_process_infer.wait()\n","\n","inference_energy_joules, inference_energy_kwh = calculate_energy_consumption(inference_power_log)\n","print(f\"Inference Energy Consumption: {inference_energy_joules:.2f} Joules ({inference_energy_kwh:.6f} kWh)\")\n","\n","# -----------------------------------------------------------------------\n","# 7) Evaluate Results (Unnormalized)\n","# -----------------------------------------------------------------------\n","predictions = preds.cpu().numpy()\n","predictions_unnorm = scaler.inverse_transform(predictions)\n","test_y_unnorm = scaler.inverse_transform(test_y.numpy())\n","\n","mse_unnorm = mean_squared_error(test_y_unnorm, predictions_unnorm)\n","rmse_unnorm = np.sqrt(mse_unnorm)\n","mae_unnorm = mean_absolute_error(test_y_unnorm, predictions_unnorm)\n","\n","print(\"Final Unnormalized Metrics (on last inference results):\")\n","print(f\" MSE:  {mse_unnorm:.6f}\")\n","print(f\" RMSE: {rmse_unnorm:.6f}\")\n","print(f\" MAE:  {mae_unnorm:.6f}\")\n","print(f\"Best Validation Loss (MSE): {best_val_loss:.6f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6--ZtQBjkQbK","executionInfo":{"status":"error","timestamp":1736800969261,"user_tz":-120,"elapsed":69256,"user":{"displayName":"bahgat ayasi","userId":"01883633993023881350"}},"outputId":"c8255d64-76f5-42ba-be40-1c37f1571e1a"},"id":"6--ZtQBjkQbK","execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-127531157a1b>:53: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  dataset.index = pd.to_datetime(dataset.index)\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Trainable parameters: 21729\n","Cleared existing log file: training_power_log.csv\n","Starting training and logging power consumption...\n","Epoch 1/60, Train Loss: 0.014300, Val Loss: 0.000739\n","Epoch 2/60, Train Loss: 0.003645, Val Loss: 0.000877\n","Epoch 3/60, Train Loss: 0.003291, Val Loss: 0.001006\n","Epoch 4/60, Train Loss: 0.003042, Val Loss: 0.000474\n","Epoch 5/60, Train Loss: 0.002969, Val Loss: 0.001270\n","Epoch 6/60, Train Loss: 0.002891, Val Loss: 0.000870\n","Epoch 7/60, Train Loss: 0.002774, Val Loss: 0.000939\n","Epoch 8/60, Train Loss: 0.002668, Val Loss: 0.000704\n","Epoch 9/60, Train Loss: 0.002698, Val Loss: 0.000638\n","Epoch 10/60, Train Loss: 0.002612, Val Loss: 0.000367\n","Epoch 11/60, Train Loss: 0.002589, Val Loss: 0.001295\n","Epoch 12/60, Train Loss: 0.002605, Val Loss: 0.002057\n","Epoch 13/60, Train Loss: 0.002614, Val Loss: 0.000700\n","Epoch 14/60, Train Loss: 0.002559, Val Loss: 0.001746\n","Epoch 15/60, Train Loss: 0.002478, Val Loss: 0.000549\n","Epoch 16/60, Train Loss: 0.002437, Val Loss: 0.000598\n","Epoch 17/60, Train Loss: 0.002454, Val Loss: 0.000754\n","Epoch 18/60, Train Loss: 0.002432, Val Loss: 0.000367\n","Epoch 19/60, Train Loss: 0.002466, Val Loss: 0.000426\n","Epoch 20/60, Train Loss: 0.002374, Val Loss: 0.000328\n","Epoch 21/60, Train Loss: 0.002417, Val Loss: 0.000580\n","Epoch 22/60, Train Loss: 0.002326, Val Loss: 0.000450\n","Epoch 23/60, Train Loss: 0.002340, Val Loss: 0.001140\n","Epoch 24/60, Train Loss: 0.002382, Val Loss: 0.000398\n","Epoch 25/60, Train Loss: 0.002341, Val Loss: 0.001239\n","Epoch 26/60, Train Loss: 0.002343, Val Loss: 0.000792\n","Epoch 27/60, Train Loss: 0.002317, Val Loss: 0.000975\n","Epoch 28/60, Train Loss: 0.002287, Val Loss: 0.000634\n","Epoch 29/60, Train Loss: 0.002246, Val Loss: 0.000449\n","Epoch 30/60, Train Loss: 0.002295, Val Loss: 0.000959\n","Epoch 31/60, Train Loss: 0.002263, Val Loss: 0.000899\n","Epoch 32/60, Train Loss: 0.002219, Val Loss: 0.000475\n","Epoch 33/60, Train Loss: 0.002280, Val Loss: 0.001026\n","Epoch 34/60, Train Loss: 0.002310, Val Loss: 0.000788\n","Epoch 35/60, Train Loss: 0.002270, Val Loss: 0.000397\n","Epoch 36/60, Train Loss: 0.002193, Val Loss: 0.000480\n","Epoch 37/60, Train Loss: 0.002232, Val Loss: 0.000332\n","Epoch 38/60, Train Loss: 0.002198, Val Loss: 0.000800\n","Epoch 39/60, Train Loss: 0.002141, Val Loss: 0.000401\n","Epoch 40/60, Train Loss: 0.002183, Val Loss: 0.000835\n","Epoch 41/60, Train Loss: 0.002184, Val Loss: 0.000801\n","Epoch 42/60, Train Loss: 0.002159, Val Loss: 0.000495\n","Epoch 43/60, Train Loss: 0.002186, Val Loss: 0.000741\n","Epoch 44/60, Train Loss: 0.002184, Val Loss: 0.000423\n","Epoch 45/60, Train Loss: 0.002178, Val Loss: 0.000564\n","Epoch 46/60, Train Loss: 0.002177, Val Loss: 0.000301\n","Epoch 47/60, Train Loss: 0.002179, Val Loss: 0.000267\n","Epoch 48/60, Train Loss: 0.002118, Val Loss: 0.000850\n","Epoch 49/60, Train Loss: 0.002103, Val Loss: 0.000565\n","Epoch 50/60, Train Loss: 0.002127, Val Loss: 0.001700\n","Epoch 51/60, Train Loss: 0.002168, Val Loss: 0.000693\n","Epoch 52/60, Train Loss: 0.002158, Val Loss: 0.000506\n","Epoch 53/60, Train Loss: 0.002116, Val Loss: 0.001563\n","Epoch 54/60, Train Loss: 0.002146, Val Loss: 0.000490\n","Epoch 55/60, Train Loss: 0.002150, Val Loss: 0.001154\n","Epoch 56/60, Train Loss: 0.002140, Val Loss: 0.000522\n","Epoch 57/60, Train Loss: 0.002200, Val Loss: 0.000531\n","Epoch 58/60, Train Loss: 0.002105, Val Loss: 0.000310\n","Epoch 59/60, Train Loss: 0.002127, Val Loss: 0.000424\n","Epoch 60/60, Train Loss: 0.002090, Val Loss: 0.000572\n","Training complete. Calculating power consumption...\n","Training Time: 64.24 sec\n","Training Energy Consumption: 2122.66 Joules (0.000590 kWh)\n","Cleared existing log file: inference_power_log.csv\n","Starting inference and logging power consumption...\n","Inference Energy Consumption: 64.58 Joules (0.000018 kWh)\n"]},{"output_type":"error","ename":"ValueError","evalue":"non-broadcastable output operand with shape (5253,1) doesn't match the broadcast shape (5253,6)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-127531157a1b>\u001b[0m in \u001b[0;36m<cell line: 243>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;31m# -----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m \u001b[0mpredictions_unnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0mtest_y_unnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    579\u001b[0m         )\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (5253,1) doesn't match the broadcast shape (5253,6)"]}]},{"cell_type":"code","source":["# LSTM Multivarite"],"metadata":{"id":"RCEu5wqWnu6j"},"id":"RCEu5wqWnu6j","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import pandas as pd\n","import time\n","import subprocess\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from pandas import read_csv\n","\n","# ---------------------------------------------------------\n","# 1) GPU power monitoring functions\n","# ---------------------------------------------------------\n","def monitor_power(output_file):\n","    \"\"\"\n","    Launch nvidia-smi in the background, logging instantaneous GPU power usage (W) every 1 second.\n","    \"\"\"\n","    command = f\"nvidia-smi --query-gpu=power.draw --format=csv -l 1 > {output_file}\"\n","    process = subprocess.Popen(command, shell=True)\n","    return process\n","\n","def calculate_energy_consumption(log_file):\n","    \"\"\"\n","    Read the CSV log with GPU power (in W) every second.\n","    Sum them up to get total Joules. Then convert Joules -> kWh.\n","    \"\"\"\n","    data = pd.read_csv(log_file, skiprows=1, names=[\"Power (W)\"])\n","    # Clean up any trailing \" W\" or similar\n","    data[\"Power (W)\"] = data[\"Power (W)\"].str.extract(r'(\\d+\\.\\d+|\\d+)').astype(float)\n","    # Energy(J) = sum of (Power_in_W * 1 second)\n","    energy_joules = data[\"Power (W)\"].sum()\n","    # Convert to kWh\n","    energy_kwh = energy_joules / (3600 * 1000)\n","    return energy_joules, energy_kwh\n","def clear_log_file(log_file):\n","    if os.path.exists(log_file):\n","        os.remove(log_file)\n","        print(f\"Cleared existing log file: {log_file}\")\n","# ---------------------------------------------------------\n","# 2) Load & preprocess data\n","# Load and preprocess the dataset\n","#dataset = pd.read_csv('Palestine-PV.csv', parse_dates=['date'], index_col='date')\n","dataset = pd.read_csv('jordan_pv.csv',  index_col='date')\n","dataset.index = pd.to_datetime(dataset.index)\n","dataset = dataset.sort_index()\n","dataset = dataset.dropna()\n","dataset = dataset.astype(np.float64)\n","\n","def series_to_supervised(data, target_col_name, n_in=1, n_out=1, dropnan=True):\n","    n_vars = data.shape[1]\n","    df = pd.DataFrame(data)\n","    cols, names = list(), list()\n","    df = df[[col for col in df if col != target_col_name] + [target_col_name]]\n","    for i in range(n_in, 0, -1):\n","        cols.append(df.shift(i))\n","        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","    for i in range(0, n_out):\n","        cols.append(df.shift(-i))\n","        if i == 0:\n","            names += [('var%d(t)' % (j+1)) for j in range(n_vars-1)]\n","            names.append(target_col_name + '(t)')\n","        else:\n","            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","    agg = pd.concat(cols, axis=1)\n","    agg.columns = names\n","    agg = agg[[col for col in agg if target_col_name in col or '(t)' not in col]]\n","    if dropnan:\n","        agg.dropna(inplace=True)\n","    return agg\n","\n","values = dataset.values\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled = scaler.fit_transform(values)\n","df_scaled = pd.DataFrame(scaled, columns=dataset.columns)\n","target_scaler = MinMaxScaler(feature_range=(0, 1))\n","target_scaled = target_scaler.fit_transform(dataset[['Solar Radiation(GHI)']])\n","reframed = series_to_supervised(df_scaled, 'Solar Radiation(GHI)', 3, 1)\n","values = reframed.values\n","n_train_hours = 876 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","train_X, train_y = train[:, :-1], train[:, -1]\n","test_X, test_y = test[:, :-1], test[:, -1]\n","train_X, train_y = torch.Tensor(train_X), torch.Tensor(train_y).view(-1, 1)\n","test_X, test_y = torch.Tensor(test_X), torch.Tensor(test_y).view(-1, 1)\n","\n","# ---------------------------------------------------------\n","# 3) Define the Conv1D-based model\n","# ---------------------------------------------------------\n","class LSTMNetwork(nn.Module):\n","    def __init__(self, input_dim, neurons, layers1, activation, dropout_rate):\n","        super(LSTMNetwork, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, neurons, layers1, batch_first=True)\n","        self.layers = nn.ModuleList()\n","        self.layers.append(nn.Linear(neurons, neurons))\n","        self.layers.append(getattr(nn, activation)())\n","        self.layers.append(nn.Dropout(p=dropout_rate))  # Dropout after first dense layer\n","        for _ in range(layers1 - 1):\n","            self.layers.append(nn.Linear(neurons, neurons))\n","            self.layers.append(getattr(nn, activation)())\n","            if dropout_rate > 0:\n","                self.layers.append(nn.Dropout(p=dropout_rate))  # Dropout after each dense layer\n","        self.output_layer = nn.Linear(neurons, 1)\n","\n","    def forward(self, x):\n","        x, _ = self.lstm(x)\n","        x = x[:, -1, :]  # get the last output of the sequence\n","        for layer in self.layers:\n","            x = layer(x)\n","        x = self.output_layer(x)\n","        return x\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# ---------------------------------------------------------\n","# 4) Hyperparams, DataLoaders, & Setup\n","# ---------------------------------------------------------\n","activation='ReLU'\n","neurons=32\n","layers1=2\n","dropout_rate=0.05\n","epochs=60\n","learning_rate=0.001\n","batch_size=64\n","\n","input_dim = train_X.shape[1]  # seq_size\n","# Reshape for conv -> shape: (N, channels=1, seq_len)\n","train_X = train_X.reshape(train_X.shape[0], 1, train_X.shape[1])\n","test_X  = test_X.reshape(test_X.shape[0], 1, test_X.shape[1])\n","\n","train_dataset = TensorDataset(train_X, train_y)\n","test_dataset  = TensorDataset(test_X, test_y)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","model = LSTMNetwork(input_dim, neurons, layers1, activation, dropout_rate).to(device)\n","nparams = count_parameters(model)\n","print(f\"Trainable parameters: {nparams}\")\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# ---------------------------------------------------------\n","# 5) TRAINING - Monitor GPU power\n","# ---------------------------------------------------------\n","# Before starting training power monitoring\n","clear_log_file(training_power_log)\n","\n","train_power_log = \"training_power_log.csv\"\n","print(\"Starting training and logging power consumption...\")\n","monitor_proc = monitor_power(train_power_log)\n","start_time = time.time()\n","\n","best_val_loss = float('inf')\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for batchX, batchY in train_loader:\n","        batchX, batchY = batchX.to(device), batchY.to(device)\n","        optimizer.zero_grad()\n","        out = model(batchX)\n","        loss = criterion(out, batchY)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    train_loss_epoch = running_loss / len(train_loader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for valX, valY in test_loader:\n","            valX, valY = valX.to(device), valY.to(device)\n","            predY = model(valX)\n","            val_loss += criterion(predY, valY).item()\n","    val_loss /= len(test_loader)\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss_epoch:.6f}, Val Loss: {val_loss:.6f}\")\n","    # Track best val\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), \"best_model.pth\")\n","\n","end_time = time.time()\n","monitor_proc.terminate()\n","monitor_proc.wait()\n","\n","train_energy_joules, train_energy_kwh = calculate_energy_consumption(train_power_log)\n","train_time = end_time - start_time\n","print(\"Training completed!\")\n","print(f\"Training Time: {train_time:.2f} sec\")\n","print(f\"Training Energy Consumption: {train_energy_joules:.2f} Joules ({train_energy_kwh:.6f} kWh)\")\n","\n","# ---------------------------------------------------------\n","# 6) INFERENCE - Monitor GPU power\n","# ---------------------------------------------------------\n","\n","# Before starting inference power monitoring\n","clear_log_file(inference_power_log)\n","\n","inference_power_log = \"inference_power_log.csv\"\n","print(\"Starting inference and logging power consumption...\")\n","monitor_proc_infer = monitor_power(inference_power_log)\n","time.sleep(1)  # give nvidia-smi time to start logging\n","\n","model.eval()\n","with torch.no_grad():\n","    # artificially enlarge the inference workload\n","    for _ in range(1):\n","        predictions = model(test_X.to(device))\n","\n","time.sleep(1)  # ensure last logs are written\n","monitor_proc_infer.terminate()\n","monitor_proc_infer.wait()\n","\n","inference_energy_joules, inference_energy_kwh = calculate_energy_consumption(inference_power_log)\n","print(\"Inference Completed!\")\n","print(f\"Inference Energy Consumption: {inference_energy_joules:.2f} Joules ({inference_energy_kwh:.6f} kWh)\")\n","\n","# ---------------------------------------------------------\n","# 7) Print out all final metrics, exactly as in the original script\n","# ---------------------------------------------------------\n","predictions_np = predictions.cpu().numpy()\n","test_y_np = test_y.cpu().numpy()\n","\n","# Inverse transform\n","pred_unnorm = scaler.inverse_transform(predictions_np)\n","test_y_unnorm = scaler.inverse_transform(test_y_np)\n","\n","# Compute MSE, RMSE, MAE on unnormalized data\n","mse_unnorm = mean_squared_error(test_y_unnorm, pred_unnorm)\n","rmse_unnorm = np.sqrt(mse_unnorm)\n","mae_unnorm = mean_absolute_error(test_y_unnorm, pred_unnorm)\n","\n","print(\"\\nFinal Unnormalized Metrics (on last inference results):\")\n","print(f\" MSE:  {mse_unnorm:.6f}\")\n","print(f\" RMSE: {rmse_unnorm:.6f}\")\n","print(f\" MAE:  {mae_unnorm:.6f}\")\n","print(f\"Best Validation Loss (MSE): {best_val_loss:.6f}\")\n"],"metadata":{"id":"5ZxC8pLTnu9Y","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1736801329938,"user_tz":-120,"elapsed":85441,"user":{"displayName":"bahgat ayasi","userId":"01883633993023881350"}},"outputId":"c3434e51-593d-443c-93dd-32822820a0cb"},"id":"5ZxC8pLTnu9Y","execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-14-e3e87ea288f1>:46: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  dataset.index = pd.to_datetime(dataset.index)\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Trainable parameters: 17249\n","Cleared existing log file: training_power_log.csv\n","Starting training and logging power consumption...\n","Epoch 1/60, Train Loss: 0.035255, Val Loss: 0.001947\n","Epoch 2/60, Train Loss: 0.004710, Val Loss: 0.001040\n","Epoch 3/60, Train Loss: 0.003953, Val Loss: 0.001456\n","Epoch 4/60, Train Loss: 0.003676, Val Loss: 0.000967\n","Epoch 5/60, Train Loss: 0.003463, Val Loss: 0.001253\n","Epoch 6/60, Train Loss: 0.003220, Val Loss: 0.000589\n","Epoch 7/60, Train Loss: 0.003088, Val Loss: 0.000682\n","Epoch 8/60, Train Loss: 0.003010, Val Loss: 0.001288\n","Epoch 9/60, Train Loss: 0.002982, Val Loss: 0.001054\n","Epoch 10/60, Train Loss: 0.002844, Val Loss: 0.001426\n","Epoch 11/60, Train Loss: 0.002870, Val Loss: 0.000614\n","Epoch 12/60, Train Loss: 0.002873, Val Loss: 0.000469\n","Epoch 13/60, Train Loss: 0.002767, Val Loss: 0.000624\n","Epoch 14/60, Train Loss: 0.002811, Val Loss: 0.000505\n","Epoch 15/60, Train Loss: 0.002745, Val Loss: 0.000707\n","Epoch 16/60, Train Loss: 0.002712, Val Loss: 0.000680\n","Epoch 17/60, Train Loss: 0.002698, Val Loss: 0.000529\n","Epoch 18/60, Train Loss: 0.002723, Val Loss: 0.000380\n","Epoch 19/60, Train Loss: 0.002678, Val Loss: 0.000347\n","Epoch 20/60, Train Loss: 0.002691, Val Loss: 0.000557\n","Epoch 21/60, Train Loss: 0.002690, Val Loss: 0.000707\n","Epoch 22/60, Train Loss: 0.002628, Val Loss: 0.000446\n","Epoch 23/60, Train Loss: 0.002619, Val Loss: 0.000362\n","Epoch 24/60, Train Loss: 0.002628, Val Loss: 0.000564\n","Epoch 25/60, Train Loss: 0.002657, Val Loss: 0.000933\n","Epoch 26/60, Train Loss: 0.002612, Val Loss: 0.000901\n","Epoch 27/60, Train Loss: 0.002648, Val Loss: 0.000345\n","Epoch 28/60, Train Loss: 0.002574, Val Loss: 0.000485\n","Epoch 29/60, Train Loss: 0.002590, Val Loss: 0.000329\n","Epoch 30/60, Train Loss: 0.002527, Val Loss: 0.000382\n","Epoch 31/60, Train Loss: 0.002656, Val Loss: 0.000424\n","Epoch 32/60, Train Loss: 0.002646, Val Loss: 0.000963\n","Epoch 33/60, Train Loss: 0.002623, Val Loss: 0.000572\n","Epoch 34/60, Train Loss: 0.002539, Val Loss: 0.000863\n","Epoch 35/60, Train Loss: 0.002599, Val Loss: 0.000509\n","Epoch 36/60, Train Loss: 0.002577, Val Loss: 0.000486\n","Epoch 37/60, Train Loss: 0.002492, Val Loss: 0.000695\n","Epoch 38/60, Train Loss: 0.002523, Val Loss: 0.000343\n","Epoch 39/60, Train Loss: 0.002499, Val Loss: 0.000623\n","Epoch 40/60, Train Loss: 0.002548, Val Loss: 0.000959\n","Epoch 41/60, Train Loss: 0.002473, Val Loss: 0.000315\n","Epoch 42/60, Train Loss: 0.002609, Val Loss: 0.000540\n","Epoch 43/60, Train Loss: 0.002503, Val Loss: 0.000396\n","Epoch 44/60, Train Loss: 0.002511, Val Loss: 0.000504\n","Epoch 45/60, Train Loss: 0.002545, Val Loss: 0.000521\n","Epoch 46/60, Train Loss: 0.002501, Val Loss: 0.000569\n","Epoch 47/60, Train Loss: 0.002574, Val Loss: 0.000615\n","Epoch 48/60, Train Loss: 0.002465, Val Loss: 0.000307\n","Epoch 49/60, Train Loss: 0.002515, Val Loss: 0.000340\n","Epoch 50/60, Train Loss: 0.002552, Val Loss: 0.000441\n","Epoch 51/60, Train Loss: 0.002467, Val Loss: 0.000558\n","Epoch 52/60, Train Loss: 0.002467, Val Loss: 0.000444\n","Epoch 53/60, Train Loss: 0.002479, Val Loss: 0.000588\n","Epoch 54/60, Train Loss: 0.002436, Val Loss: 0.000577\n","Epoch 55/60, Train Loss: 0.002442, Val Loss: 0.000670\n","Epoch 56/60, Train Loss: 0.002457, Val Loss: 0.000637\n","Epoch 57/60, Train Loss: 0.002524, Val Loss: 0.000590\n","Epoch 58/60, Train Loss: 0.002408, Val Loss: 0.002086\n","Epoch 59/60, Train Loss: 0.002420, Val Loss: 0.000406\n","Epoch 60/60, Train Loss: 0.002441, Val Loss: 0.000670\n","Training completed!\n","Training Time: 79.37 sec\n","Training Energy Consumption: 2601.61 Joules (0.000723 kWh)\n","Cleared existing log file: inference_power_log.csv\n","Starting inference and logging power consumption...\n","Inference Completed!\n","Inference Energy Consumption: 65.47 Joules (0.000018 kWh)\n"]},{"output_type":"error","ename":"ValueError","evalue":"non-broadcastable output operand with shape (5253,1) doesn't match the broadcast shape (5253,6)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-e3e87ea288f1>\u001b[0m in \u001b[0;36m<cell line: 234>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;31m# Inverse transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m \u001b[0mpred_unnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0mtest_y_unnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    579\u001b[0m         )\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (5253,1) doesn't match the broadcast shape (5253,6)"]}]},{"cell_type":"code","source":[],"metadata":{"id":"AawHIIZmnvAJ"},"id":"AawHIIZmnvAJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f83InbdGnvC-"},"id":"f83InbdGnvC-","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}