{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc8005dd",
   "metadata": {
    "id": "fc8005dd"
   },
   "source": [
    "# ANN- Univariate Time Series Solar Radiation(GHI) Forecasting with MLP, CNN, and LSTM Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db958e-c7fb-4950-8406-b3fe5b41a604",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d57f75-b81b-41fc-b63b-a074c4fefe13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21020, 3]) torch.Size([21020, 1]) torch.Size([5252, 3]) torch.Size([5252, 1])\n",
      "Trainable parameters: 2273\n",
      "layers.0.weight: 96 parameters\n",
      "layers.0.bias: 32 parameters\n",
      "layers.2.weight: 1024 parameters\n",
      "layers.2.bias: 32 parameters\n",
      "layers.5.weight: 1024 parameters\n",
      "layers.5.bias: 32 parameters\n",
      "layers.8.weight: 32 parameters\n",
      "layers.8.bias: 1 parameters\n",
      "Epoch 1/60, Loss: 0.007131, Val Loss: 0.004748\n",
      "Epoch 2/60, Loss: 0.006280, Val Loss: 0.004035\n",
      "Epoch 3/60, Loss: 0.008248, Val Loss: 0.003617\n",
      "Epoch 4/60, Loss: 0.007108, Val Loss: 0.003482\n",
      "Epoch 5/60, Loss: 0.003914, Val Loss: 0.003495\n",
      "Epoch 6/60, Loss: 0.000936, Val Loss: 0.003557\n",
      "Epoch 7/60, Loss: 0.021942, Val Loss: 0.003463\n",
      "Epoch 8/60, Loss: 0.002667, Val Loss: 0.003475\n",
      "Epoch 9/60, Loss: 0.005949, Val Loss: 0.003257\n",
      "Epoch 10/60, Loss: 0.014127, Val Loss: 0.003131\n",
      "Epoch 11/60, Loss: 0.007826, Val Loss: 0.003143\n",
      "Epoch 12/60, Loss: 0.002152, Val Loss: 0.003623\n",
      "Epoch 13/60, Loss: 0.002792, Val Loss: 0.003076\n",
      "Epoch 14/60, Loss: 0.001154, Val Loss: 0.003020\n",
      "Epoch 15/60, Loss: 0.009141, Val Loss: 0.003148\n",
      "Epoch 16/60, Loss: 0.004649, Val Loss: 0.003348\n",
      "Epoch 17/60, Loss: 0.003952, Val Loss: 0.003229\n",
      "Epoch 18/60, Loss: 0.012412, Val Loss: 0.003122\n",
      "Epoch 19/60, Loss: 0.003346, Val Loss: 0.003184\n",
      "Epoch 20/60, Loss: 0.007431, Val Loss: 0.003041\n",
      "Epoch 21/60, Loss: 0.001448, Val Loss: 0.003128\n",
      "Epoch 22/60, Loss: 0.006854, Val Loss: 0.003125\n",
      "Epoch 23/60, Loss: 0.007513, Val Loss: 0.003058\n",
      "Epoch 24/60, Loss: 0.012679, Val Loss: 0.003385\n",
      "Epoch 25/60, Loss: 0.004659, Val Loss: 0.003056\n",
      "Epoch 26/60, Loss: 0.013294, Val Loss: 0.003018\n",
      "Epoch 27/60, Loss: 0.003557, Val Loss: 0.003013\n",
      "Epoch 28/60, Loss: 0.007423, Val Loss: 0.002997\n",
      "Epoch 29/60, Loss: 0.008161, Val Loss: 0.003209\n",
      "Epoch 30/60, Loss: 0.007468, Val Loss: 0.002944\n",
      "Epoch 31/60, Loss: 0.004105, Val Loss: 0.003130\n",
      "Epoch 32/60, Loss: 0.002925, Val Loss: 0.003062\n",
      "Epoch 33/60, Loss: 0.001475, Val Loss: 0.003469\n",
      "Epoch 34/60, Loss: 0.003284, Val Loss: 0.003039\n",
      "Epoch 35/60, Loss: 0.003201, Val Loss: 0.002958\n",
      "Epoch 36/60, Loss: 0.004830, Val Loss: 0.003432\n",
      "Epoch 37/60, Loss: 0.006489, Val Loss: 0.003034\n",
      "Epoch 38/60, Loss: 0.005360, Val Loss: 0.003040\n",
      "Epoch 39/60, Loss: 0.002711, Val Loss: 0.003765\n",
      "Epoch 40/60, Loss: 0.007986, Val Loss: 0.003438\n",
      "Epoch 41/60, Loss: 0.003354, Val Loss: 0.003877\n",
      "Epoch 42/60, Loss: 0.001951, Val Loss: 0.002894\n",
      "Epoch 43/60, Loss: 0.001748, Val Loss: 0.002998\n",
      "Epoch 44/60, Loss: 0.015583, Val Loss: 0.003075\n",
      "Epoch 45/60, Loss: 0.006814, Val Loss: 0.003063\n",
      "Epoch 46/60, Loss: 0.001955, Val Loss: 0.003167\n",
      "Epoch 47/60, Loss: 0.004051, Val Loss: 0.002904\n",
      "Epoch 48/60, Loss: 0.010590, Val Loss: 0.003120\n",
      "Epoch 49/60, Loss: 0.003006, Val Loss: 0.003082\n",
      "Epoch 50/60, Loss: 0.002939, Val Loss: 0.003006\n",
      "Epoch 51/60, Loss: 0.003043, Val Loss: 0.003099\n",
      "Epoch 52/60, Loss: 0.006246, Val Loss: 0.002983\n",
      "Epoch 53/60, Loss: 0.006256, Val Loss: 0.003314\n",
      "Epoch 54/60, Loss: 0.003743, Val Loss: 0.003017\n",
      "Epoch 55/60, Loss: 0.010535, Val Loss: 0.003442\n",
      "Epoch 56/60, Loss: 0.006999, Val Loss: 0.002924\n",
      "Epoch 57/60, Loss: 0.010019, Val Loss: 0.003273\n",
      "Epoch 58/60, Loss: 0.010106, Val Loss: 0.002934\n",
      "Epoch 59/60, Loss: 0.013254, Val Loss: 0.002891\n",
      "Epoch 60/60, Loss: 0.007322, Val Loss: 0.002941\n",
      "Minimum Validation Loss (MSE): 0.002891\n",
      "Square of Minimum Validation Loss: 0.053768\n",
      "Mean Squared Error (MSE): 0.002975\n",
      "Root Mean Squared Error (RMSE): 0.054544\n",
      "Unnormalized Mean Absolute Error (MAE): 24.121363\n",
      "Unnormalized Mean Sequare Error (MSE): 3292.482178\n",
      "Unnormalized Root Mean Sequare Error (RMSE): 57.380154\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "#data = read_csv('jordan_pv.csv', parse_dates=['date'], index_col='date')\n",
    "data = read_csv('Palestine-PV.csv', parse_dates=['date'],  index_col='date')\n",
    "\n",
    "# Convert index to datetime\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# Sort the DataFrame by the index (date) in ascending order to ensure oldest to newest\n",
    "data = data.sort_index()\n",
    "\n",
    "# Manually specify column names\n",
    "# Now select the first column after the index\n",
    "data = data.iloc[:, 5] \n",
    "\n",
    "data = data[~np.isnan(data)]\n",
    "data = data.dropna()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "# Convert the Series to a numpy array and reshape\n",
    "data = np.array(data).reshape(-1, 1)\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(scaled_data) * 0.80)\n",
    "test_size = len(scaled_data) - train_size\n",
    "train, test = scaled_data[0:train_size,:], scaled_data[train_size:len(scaled_data),:]\n",
    "\n",
    "def to_sequences(dataset, seq_size=1):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(dataset)-seq_size-1):\n",
    "        window = dataset[i:(i+seq_size), 0]\n",
    "        x.append(window)\n",
    "        y.append(dataset[i+seq_size, 0])\n",
    "        \n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "seq_size = 3  # Number of time steps to look back \n",
    "\n",
    "train_X, train_y = to_sequences(train, seq_size)\n",
    "test_X, test_y = to_sequences(test, seq_size)\n",
    "train_X, train_y = torch.tensor(train_X).float(), torch.tensor(train_y).view(-1, 1).float()\n",
    "test_X, test_y = torch.tensor(test_X).float(), torch.tensor(test_y).view(-1, 1).float()\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)  # Corrected to print test_y.shape\n",
    "\n",
    "# Define the model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, neurons, layers1, activation, dropout_rate):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, neurons))\n",
    "        self.layers.append(getattr(nn, activation)())\n",
    "        for _ in range(layers1):\n",
    "            self.layers.append(nn.Linear(neurons, neurons))\n",
    "            self.layers.append(getattr(nn, activation)())\n",
    "            if dropout_rate > 0:\n",
    "                self.layers.append(nn.Dropout(p=dropout_rate))\n",
    "        self.layers.append(nn.Linear(neurons, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Function to count the trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Parameters\n",
    "activation = 'ReLU'\n",
    "batch_size = 64\n",
    "dropout_rate = 0.05\n",
    "epochs = 60\n",
    "layers1 = 2\n",
    "learning_rate = 0.001\n",
    "neurons = 32\n",
    "input_dim = train_X.shape[1]\n",
    "\n",
    "# Prepare the data\n",
    "train_dataset = TensorDataset(train_X, train_y)\n",
    "test_dataset = TensorDataset(test_X, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # Changed shuffle to False for validation\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = NeuralNetwork(input_dim, neurons, layers1, activation, dropout_rate)\n",
    "model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "# Print trainable parameters\n",
    "print(f\"Trainable parameters: {count_parameters(model)}\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.numel()} parameters\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(model.device if hasattr(model, 'device') else torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        batch_y = batch_y.to(model.device if hasattr(model, 'device') else torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for val_X, val_y in test_loader:\n",
    "            val_X = val_X.to(model.device if hasattr(model, 'device') else torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            val_y = val_y.to(model.device if hasattr(model, 'device') else torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            val_outputs = model(val_X)\n",
    "            val_loss += criterion(val_outputs, val_y).item()\n",
    "    val_loss /= len(test_loader)\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}, Val Loss: {val_loss:.6f}')\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "\n",
    "# Print the minimum validation loss\n",
    "print(f\"Minimum Validation Loss (MSE): {min_val_loss:.6f}\")\n",
    "\n",
    "# Compute the square of the minimum validation loss\n",
    "squared_min_val_loss = np.sqrt(min_val_loss )\n",
    "print(f\"Square of Minimum Validation Loss: {squared_min_val_loss:.6f}\")\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_X.to(model.device if hasattr(model, 'device') else torch.device('cuda' if torch.cuda.is_available() else 'cpu'))).cpu().numpy()\n",
    "\n",
    "# Inverse transform the scaled predictions and true values to original scale\n",
    "predictions_unnorm = scaler.inverse_transform(predictions)\n",
    "test_y_unnorm = scaler.inverse_transform(test_y.numpy())\n",
    "\n",
    "# Evaluate the model\n",
    "mse = criterion(torch.tensor(predictions), test_y).item()\n",
    "print(\"Mean Squared Error (MSE): {:.6f}\".format(mse))\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE): {:.6f}\".format(rmse))\n",
    "\n",
    "# Calculate Unnormalized MAE\n",
    "mae = mean_absolute_error(test_y_unnorm, predictions_unnorm)\n",
    "print(\"Unnormalized Mean Absolute Error (MAE): {:.6f}\".format(mae))\n",
    "\n",
    "# Calculate Unnormalized MSE\n",
    "mse = mean_squared_error(test_y_unnorm, predictions_unnorm)\n",
    "rmse=np.sqrt(mse)\n",
    "print(\"Unnormalized Mean Sequare Error (MSE): {:.6f}\".format(mse))\n",
    "print(\"Unnormalized Root Mean Sequare Error (RMSE): {:.6f}\".format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c816a481-78f5-481c-b62b-8159d317a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4886c4a",
   "metadata": {
    "id": "f4886c4a"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55983fbd-3082-453f-8b41-4608fbaa8aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21020, 3]) torch.Size([21020, 1]) torch.Size([5252, 3]) torch.Size([5252, 1])\n",
      "Trainable parameters: 6369\n",
      "conv1.weight: 32 parameters\n",
      "conv1.bias: 32 parameters\n",
      "conv2.weight: 1024 parameters\n",
      "conv2.bias: 32 parameters\n",
      "layers.0.weight: 3072 parameters\n",
      "layers.0.bias: 32 parameters\n",
      "layers.2.weight: 1024 parameters\n",
      "layers.2.bias: 32 parameters\n",
      "layers.5.weight: 1024 parameters\n",
      "layers.5.bias: 32 parameters\n",
      "output_layer.weight: 32 parameters\n",
      "output_layer.bias: 1 parameters\n",
      "Epoch 1/60, Loss: 0.009286, Val Loss: 0.004284\n",
      "Epoch 2/60, Loss: 0.004711, Val Loss: 0.004110\n",
      "Epoch 3/60, Loss: 0.004137, Val Loss: 0.003797\n",
      "Epoch 4/60, Loss: 0.005937, Val Loss: 0.003277\n",
      "Epoch 5/60, Loss: 0.008613, Val Loss: 0.004457\n",
      "Epoch 6/60, Loss: 0.004802, Val Loss: 0.003541\n",
      "Epoch 7/60, Loss: 0.007850, Val Loss: 0.003375\n",
      "Epoch 8/60, Loss: 0.008295, Val Loss: 0.003197\n",
      "Epoch 9/60, Loss: 0.003011, Val Loss: 0.003581\n",
      "Epoch 10/60, Loss: 0.011912, Val Loss: 0.004669\n",
      "Epoch 11/60, Loss: 0.008284, Val Loss: 0.003427\n",
      "Epoch 12/60, Loss: 0.006564, Val Loss: 0.003819\n",
      "Epoch 13/60, Loss: 0.003809, Val Loss: 0.003355\n",
      "Epoch 14/60, Loss: 0.005822, Val Loss: 0.003492\n",
      "Epoch 15/60, Loss: 0.003671, Val Loss: 0.003302\n",
      "Epoch 16/60, Loss: 0.005838, Val Loss: 0.003058\n",
      "Epoch 17/60, Loss: 0.018537, Val Loss: 0.003184\n",
      "Epoch 18/60, Loss: 0.005806, Val Loss: 0.003395\n",
      "Epoch 19/60, Loss: 0.007120, Val Loss: 0.004183\n",
      "Epoch 20/60, Loss: 0.004007, Val Loss: 0.003004\n",
      "Epoch 21/60, Loss: 0.008810, Val Loss: 0.003154\n",
      "Epoch 22/60, Loss: 0.000932, Val Loss: 0.003026\n",
      "Epoch 23/60, Loss: 0.011543, Val Loss: 0.003998\n",
      "Epoch 24/60, Loss: 0.007426, Val Loss: 0.003379\n",
      "Epoch 25/60, Loss: 0.001452, Val Loss: 0.003895\n",
      "Epoch 26/60, Loss: 0.004922, Val Loss: 0.003201\n",
      "Epoch 27/60, Loss: 0.003988, Val Loss: 0.003527\n",
      "Epoch 28/60, Loss: 0.005488, Val Loss: 0.003118\n",
      "Epoch 29/60, Loss: 0.006113, Val Loss: 0.003585\n",
      "Epoch 30/60, Loss: 0.004627, Val Loss: 0.003560\n",
      "Epoch 31/60, Loss: 0.005027, Val Loss: 0.003193\n",
      "Epoch 32/60, Loss: 0.005046, Val Loss: 0.003089\n",
      "Epoch 33/60, Loss: 0.003521, Val Loss: 0.003274\n",
      "Epoch 34/60, Loss: 0.014248, Val Loss: 0.003595\n",
      "Epoch 35/60, Loss: 0.012684, Val Loss: 0.003669\n",
      "Epoch 36/60, Loss: 0.001821, Val Loss: 0.003357\n",
      "Epoch 37/60, Loss: 0.004124, Val Loss: 0.003660\n",
      "Epoch 38/60, Loss: 0.001908, Val Loss: 0.003213\n",
      "Epoch 39/60, Loss: 0.011520, Val Loss: 0.003018\n",
      "Epoch 40/60, Loss: 0.001333, Val Loss: 0.003310\n",
      "Epoch 41/60, Loss: 0.005181, Val Loss: 0.003336\n",
      "Epoch 42/60, Loss: 0.001616, Val Loss: 0.003790\n",
      "Epoch 43/60, Loss: 0.002314, Val Loss: 0.003101\n",
      "Epoch 44/60, Loss: 0.004159, Val Loss: 0.003537\n",
      "Epoch 45/60, Loss: 0.007299, Val Loss: 0.003312\n",
      "Epoch 46/60, Loss: 0.001991, Val Loss: 0.003126\n",
      "Epoch 47/60, Loss: 0.003986, Val Loss: 0.003316\n",
      "Epoch 48/60, Loss: 0.002754, Val Loss: 0.003578\n",
      "Epoch 49/60, Loss: 0.007807, Val Loss: 0.003595\n",
      "Epoch 50/60, Loss: 0.000773, Val Loss: 0.003190\n",
      "Epoch 51/60, Loss: 0.006597, Val Loss: 0.003341\n",
      "Epoch 52/60, Loss: 0.008099, Val Loss: 0.003162\n",
      "Epoch 53/60, Loss: 0.006340, Val Loss: 0.004579\n",
      "Epoch 54/60, Loss: 0.002035, Val Loss: 0.002964\n",
      "Epoch 55/60, Loss: 0.003508, Val Loss: 0.003486\n",
      "Epoch 56/60, Loss: 0.001251, Val Loss: 0.003191\n",
      "Epoch 57/60, Loss: 0.003499, Val Loss: 0.002967\n",
      "Epoch 58/60, Loss: 0.004898, Val Loss: 0.002850\n",
      "Epoch 59/60, Loss: 0.007040, Val Loss: 0.003829\n",
      "Epoch 60/60, Loss: 0.006424, Val Loss: 0.002966\n",
      "The training process took 186.90 seconds.\n",
      "Minimum Validation Loss (MSE): 0.002850\n",
      "Root Mean Squared Error of Minimum Validation Loss: 0.053384\n",
      "Mean Squared Error (MSE) on Test Set (Normalized): 0.002999\n",
      "Root Mean Squared Error (RMSE) on Test Set (Normalized): 0.054767\n",
      "Mean Squared Error (MSE) on Test Set (Unnormalized): 3319.423096\n",
      "Root Mean Squared Error (RMSE) on Test Set (Unnormalized): 57.614433\n",
      "Mean Absolute Error (MAE) on Test Set (Unnormalized): 27.896866\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error  # Added for MAE and MSE\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "#data = read_csv('jordan_pv.csv', parse_dates=['date'], index_col='date')\n",
    "data = read_csv('Palestine-PV.csv', parse_dates=['date'],  index_col='date')\n",
    "\n",
    "# Convert index to datetime\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# Sort the DataFrame by the index (date) in ascending order to ensure oldest to newest\n",
    "data = data.sort_index()\n",
    "\n",
    "# Sort the DataFrame by the index (date) in ascending order to ensure oldest to newest\n",
    "data = data.sort_index()\n",
    "\n",
    "# Manually specify column names\n",
    "# Now select the sixth column (index 5) after the index\n",
    "data = data.iloc[:, 5] \n",
    "\n",
    "# Remove NaN values\n",
    "data = data[~np.isnan(data)]\n",
    "data = data.dropna()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "# Convert the Series to a numpy array and reshape\n",
    "data = np.array(data).reshape(-1, 1)\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(scaled_data) * 0.80)\n",
    "test_size = len(scaled_data) - train_size\n",
    "train, test = scaled_data[0:train_size, :], scaled_data[train_size:len(scaled_data), :]\n",
    "\n",
    "def to_sequences(dataset, seq_size=1):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(dataset) - seq_size - 1):\n",
    "        window = dataset[i:(i + seq_size), 0]\n",
    "        x.append(window)\n",
    "        y.append(dataset[i + seq_size, 0])\n",
    "        \n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "seq_size = 3  # Number of time steps to look back \n",
    "\n",
    "train_X, train_y = to_sequences(train, seq_size)\n",
    "test_X, test_y = to_sequences(test, seq_size)\n",
    "\n",
    "train_X, train_y = torch.tensor(train_X).float(), torch.tensor(train_y).view(-1, 1).float()\n",
    "test_X, test_y = torch.tensor(test_X).float(), torch.tensor(test_y).view(-1, 1).float()\n",
    "\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)  # Corrected to print test_y.shape\n",
    "\n",
    "# Define the model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, neurons, layers1, activation, dropout_rate):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim * 32, neurons))\n",
    "        self.layers.append(getattr(nn, activation)())\n",
    "        for _ in range(layers1):\n",
    "            self.layers.append(nn.Linear(neurons, neurons))\n",
    "            self.layers.append(getattr(nn, activation)())\n",
    "            if dropout_rate > 0:\n",
    "                self.layers.append(nn.Dropout(p=dropout_rate))\n",
    "        self.output_layer = nn.Linear(neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Function to count the trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Parameters\n",
    "activation = 'ReLU'\n",
    "batch_size = 64\n",
    "dropout_rate = 0.05\n",
    "epochs = 60\n",
    "layers1 = 2\n",
    "learning_rate = 0.001\n",
    "neurons = 32\n",
    "input_dim = train_X.shape[1]\n",
    "\n",
    "# Prepare the data\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))  # (batch_size, channels, seq_length)\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "train_dataset = TensorDataset(train_X, train_y)\n",
    "test_dataset = TensorDataset(test_X, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # Changed shuffle to False for validation\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = NeuralNetwork(input_dim, neurons, layers1, activation, dropout_rate)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(f\"Trainable parameters: {count_parameters(model)}\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.numel()} parameters\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = float('inf')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for val_X, val_y in test_loader:\n",
    "            val_X, val_y = val_X.to(device), val_y.to(device)\n",
    "            val_outputs = model(val_X)\n",
    "            val_loss += criterion(val_outputs, val_y).item()\n",
    "    val_loss /= len(test_loader)\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}, Val Loss: {val_loss:.6f}')\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The training process took {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "# Print the minimum validation loss\n",
    "print(f\"Minimum Validation Loss (MSE): {min_val_loss:.6f}\")\n",
    "\n",
    "# Compute the RMSE of the minimum validation loss\n",
    "rmse_min_val_loss = np.sqrt(min_val_loss)\n",
    "print(f\"Root Mean Squared Error of Minimum Validation Loss: {rmse_min_val_loss:.6f}\")\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_X.to(device)).cpu().numpy()\n",
    "\n",
    "# Inverse transform the scaled predictions and true values to original scale\n",
    "predictions_unnorm = scaler.inverse_transform(predictions)\n",
    "test_y_unnorm = scaler.inverse_transform(test_y.numpy())\n",
    "\n",
    "# Evaluate the model on unnormalized data\n",
    "mse_unnorm = mean_squared_error(test_y_unnorm, predictions_unnorm)\n",
    "rmse_unnorm = np.sqrt(mse_unnorm)\n",
    "mae_unnorm = mean_absolute_error(test_y_unnorm, predictions_unnorm)\n",
    "\n",
    "print(\"Mean Squared Error (MSE) on Test Set (Normalized): {:.6f}\".format(mean_squared_error(test_y.numpy(), predictions)))\n",
    "print(\"Root Mean Squared Error (RMSE) on Test Set (Normalized): {:.6f}\".format(np.sqrt(mean_squared_error(test_y.numpy(), predictions))))\n",
    "print(\"Mean Squared Error (MSE) on Test Set (Unnormalized): {:.6f}\".format(mse_unnorm))\n",
    "print(\"Root Mean Squared Error (RMSE) on Test Set (Unnormalized): {:.6f}\".format(rmse_unnorm))\n",
    "print(\"Mean Absolute Error (MAE) on Test Set (Unnormalized): {:.6f}\".format(mae_unnorm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e641005-ff8e-4664-9353-3c80f1b1f120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(1,), stride=(1,))\n",
      "  (conv2): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=576, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (output_layer): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc2438-75b0-4caf-923d-185e5156ae7f",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c86810d0-08e5-47e6-85c2-9fee5953305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21020, 3]) torch.Size([21020, 1]) torch.Size([5252, 3]) torch.Size([5252, 1])\n",
      "Trainable parameters: 6881\n",
      "lstm.weight_ih_l0: 384 parameters\n",
      "lstm.weight_hh_l0: 4096 parameters\n",
      "lstm.bias_ih_l0: 128 parameters\n",
      "lstm.bias_hh_l0: 128 parameters\n",
      "layers.0.weight: 1024 parameters\n",
      "layers.0.bias: 32 parameters\n",
      "layers.2.weight: 1024 parameters\n",
      "layers.2.bias: 32 parameters\n",
      "output_layer.weight: 32 parameters\n",
      "output_layer.bias: 1 parameters\n",
      "Epoch 1/60, Loss: 0.003236, Val Loss: 0.004833\n",
      "Epoch 2/60, Loss: 0.014932, Val Loss: 0.003983\n",
      "Epoch 3/60, Loss: 0.015693, Val Loss: 0.003664\n",
      "Epoch 4/60, Loss: 0.002514, Val Loss: 0.003636\n",
      "Epoch 5/60, Loss: 0.006996, Val Loss: 0.003550\n",
      "Epoch 6/60, Loss: 0.008389, Val Loss: 0.003586\n",
      "Epoch 7/60, Loss: 0.002899, Val Loss: 0.003528\n",
      "Epoch 8/60, Loss: 0.008473, Val Loss: 0.003407\n",
      "Epoch 9/60, Loss: 0.005718, Val Loss: 0.003383\n",
      "Epoch 10/60, Loss: 0.005131, Val Loss: 0.003392\n",
      "Epoch 11/60, Loss: 0.004763, Val Loss: 0.003678\n",
      "Epoch 12/60, Loss: 0.002175, Val Loss: 0.003741\n",
      "Epoch 13/60, Loss: 0.013464, Val Loss: 0.003565\n",
      "Epoch 14/60, Loss: 0.008191, Val Loss: 0.003441\n",
      "Epoch 15/60, Loss: 0.016843, Val Loss: 0.003440\n",
      "Epoch 16/60, Loss: 0.009539, Val Loss: 0.003185\n",
      "Epoch 17/60, Loss: 0.003796, Val Loss: 0.003212\n",
      "Epoch 18/60, Loss: 0.003694, Val Loss: 0.003367\n",
      "Epoch 19/60, Loss: 0.003677, Val Loss: 0.003656\n",
      "Epoch 20/60, Loss: 0.010633, Val Loss: 0.003314\n",
      "Epoch 21/60, Loss: 0.001089, Val Loss: 0.003572\n",
      "Epoch 22/60, Loss: 0.003931, Val Loss: 0.003206\n",
      "Epoch 23/60, Loss: 0.003456, Val Loss: 0.003510\n",
      "Epoch 24/60, Loss: 0.008100, Val Loss: 0.003197\n",
      "Epoch 25/60, Loss: 0.008834, Val Loss: 0.003169\n",
      "Epoch 26/60, Loss: 0.024264, Val Loss: 0.003204\n",
      "Epoch 27/60, Loss: 0.018813, Val Loss: 0.003225\n",
      "Epoch 28/60, Loss: 0.001811, Val Loss: 0.003388\n",
      "Epoch 29/60, Loss: 0.004086, Val Loss: 0.003139\n",
      "Epoch 30/60, Loss: 0.006483, Val Loss: 0.003448\n",
      "Epoch 31/60, Loss: 0.012053, Val Loss: 0.003326\n",
      "Epoch 32/60, Loss: 0.008311, Val Loss: 0.003295\n",
      "Epoch 33/60, Loss: 0.004748, Val Loss: 0.003057\n",
      "Epoch 34/60, Loss: 0.006501, Val Loss: 0.003172\n",
      "Epoch 35/60, Loss: 0.018667, Val Loss: 0.003277\n",
      "Epoch 36/60, Loss: 0.001570, Val Loss: 0.003196\n",
      "Epoch 37/60, Loss: 0.002799, Val Loss: 0.002992\n",
      "Epoch 38/60, Loss: 0.003816, Val Loss: 0.003115\n",
      "Epoch 39/60, Loss: 0.004215, Val Loss: 0.003087\n",
      "Epoch 40/60, Loss: 0.007271, Val Loss: 0.003066\n",
      "Epoch 41/60, Loss: 0.011986, Val Loss: 0.003018\n",
      "Epoch 42/60, Loss: 0.005858, Val Loss: 0.003235\n",
      "Epoch 43/60, Loss: 0.003508, Val Loss: 0.003084\n",
      "Epoch 44/60, Loss: 0.018140, Val Loss: 0.003397\n",
      "Epoch 45/60, Loss: 0.002665, Val Loss: 0.003377\n",
      "Epoch 46/60, Loss: 0.004826, Val Loss: 0.002991\n",
      "Epoch 47/60, Loss: 0.007660, Val Loss: 0.003015\n",
      "Epoch 48/60, Loss: 0.004600, Val Loss: 0.003232\n",
      "Epoch 49/60, Loss: 0.003758, Val Loss: 0.003356\n",
      "Epoch 50/60, Loss: 0.001519, Val Loss: 0.002973\n",
      "Epoch 51/60, Loss: 0.012306, Val Loss: 0.003162\n",
      "Epoch 52/60, Loss: 0.005925, Val Loss: 0.003519\n",
      "Epoch 53/60, Loss: 0.000880, Val Loss: 0.003285\n",
      "Epoch 54/60, Loss: 0.014422, Val Loss: 0.003110\n",
      "Epoch 55/60, Loss: 0.006456, Val Loss: 0.003126\n",
      "Epoch 56/60, Loss: 0.007585, Val Loss: 0.003146\n",
      "Epoch 57/60, Loss: 0.005180, Val Loss: 0.003037\n",
      "Epoch 58/60, Loss: 0.005457, Val Loss: 0.003056\n",
      "Epoch 59/60, Loss: 0.002014, Val Loss: 0.003113\n",
      "Epoch 60/60, Loss: 0.002488, Val Loss: 0.003057\n",
      "Minimum Validation Loss (MSE): 0.002973\n",
      "Unnormalized Root Mean Squared Error of Minimum Validation Loss: 57.363422\n",
      "Mean Squared Error (MSE) on Test Set (Normalized): 0.003091\n",
      "Root Mean Squared Error (RMSE) on Test Set (Normalized): 0.055597\n",
      "Mean Squared Error (MSE) on Test Set (Unnormalized): 3420.865234\n",
      "Root Mean Squared Error (RMSE) on Test Set (Unnormalized): 58.488163\n",
      "Mean Absolute Error (MAE) on Test Set (Unnormalized): 28.970785\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error  # Added for MAE and MSE\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "#data = read_csv('jordan_pv.csv', parse_dates=['date'], index_col='date')\n",
    "data = read_csv('Palestine-PV.csv', parse_dates=['date'],  index_col='date')\n",
    "\n",
    "# Convert index to datetime\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# Sort the DataFrame by the index (date) in ascending order to ensure oldest to newest\n",
    "data = data.sort_index()\n",
    "\n",
    "# Sort the DataFrame by the index (date) in ascending order to ensure oldest to newest\n",
    "data = data.sort_index()\n",
    "# Manually specify column names\n",
    "# Now select the sixth column (index 5) after the index\n",
    "data = data.iloc[:, 5]\n",
    "\n",
    "# Remove NaN values\n",
    "data = data[~np.isnan(data)]\n",
    "data = data.dropna()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "# Convert the Series to a numpy array and reshape\n",
    "data = np.array(data).reshape(-1, 1)\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(scaled_data) * 0.80)\n",
    "test_size = len(scaled_data) - train_size\n",
    "train, test = scaled_data[0:train_size, :], scaled_data[train_size:len(scaled_data), :]\n",
    "\n",
    "def to_sequences(dataset, seq_size=1):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(dataset) - seq_size - 1):\n",
    "        window = dataset[i:(i + seq_size), 0]\n",
    "        x.append(window)\n",
    "        y.append(dataset[i + seq_size, 0])\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "seq_size = 3  # Number of time steps to look back \n",
    "\n",
    "train_X, train_y = to_sequences(train, seq_size)\n",
    "test_X, test_y = to_sequences(test, seq_size)\n",
    "\n",
    "train_X, train_y = torch.tensor(train_X).float(), torch.tensor(train_y).view(-1, 1).float()\n",
    "test_X, test_y = torch.tensor(test_X).float(), torch.tensor(test_y).view(-1, 1).float()\n",
    "\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)  # Corrected to print test_y.shape\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, neurons, layers1, activation, dropout_rate):\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, neurons, batch_first=True)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(neurons, neurons))\n",
    "        self.layers.append(getattr(nn, activation)())\n",
    "        for _ in range(layers1 - 1):\n",
    "            self.layers.append(nn.Linear(neurons, neurons))\n",
    "            self.layers.append(getattr(nn, activation)())\n",
    "            if dropout_rate > 0:\n",
    "                self.layers.append(nn.Dropout(p=dropout_rate))\n",
    "        self.output_layer = nn.Linear(neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # get the last output of the sequence\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Function to count the trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Parameters\n",
    "activation = 'ReLU'\n",
    "batch_size = 64\n",
    "dropout_rate = 0.05\n",
    "epochs = 60\n",
    "layers1 = 2\n",
    "learning_rate = 0.001\n",
    "neurons = 32\n",
    "input_dim = train_X.shape[1]\n",
    "\n",
    "# Prepare the data\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))  # (batch_size, channels, seq_length)\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "train_dataset = TensorDataset(train_X, train_y)\n",
    "test_dataset = TensorDataset(test_X, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # Changed shuffle to False for validation\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMNetwork(input_dim, neurons, layers1, activation, dropout_rate)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(f\"Trainable parameters: {count_parameters(model)}\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.numel()} parameters\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for val_X, val_y in test_loader:\n",
    "            val_X, val_y = val_X.to(device), val_y.to(device)\n",
    "            val_outputs = model(val_X)\n",
    "            val_loss += criterion(val_outputs, val_y).item()\n",
    "    val_loss /= len(test_loader)\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}, Val Loss: {val_loss:.6f}')\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "\n",
    "\n",
    "# Print the minimum validation loss\n",
    "print(f\"Minimum Validation Loss (MSE): {min_val_loss:.6f}\")\n",
    "\n",
    "# Compute the RMSE of the minimum validation loss\n",
    "# Since min_val_loss is in MSE (normalized), we need to convert it to the original scale\n",
    "# RMSE_original = sqrt(MSE_normalized) * (x_max - x_min)\n",
    "scale = scaler.data_max_ - scaler.data_min_\n",
    "rmse_min_val_loss_unnorm = np.sqrt(min_val_loss) * scale\n",
    "print(f\"Unnormalized Root Mean Squared Error of Minimum Validation Loss: {rmse_min_val_loss_unnorm[0]:.6f}\")\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_X.to(device)).cpu().numpy()\n",
    "\n",
    "# Inverse transform the scaled predictions and true values to original scale\n",
    "predictions_unnorm = scaler.inverse_transform(predictions)\n",
    "test_y_unnorm = scaler.inverse_transform(test_y.numpy())\n",
    "\n",
    "# Evaluate the model on normalized data\n",
    "mse_normalized = mean_squared_error(test_y.numpy(), predictions)\n",
    "rmse_normalized = np.sqrt(mse_normalized)\n",
    "\n",
    "# Evaluate the model on unnormalized data\n",
    "mse_unnorm = mean_squared_error(test_y_unnorm, predictions_unnorm)\n",
    "rmse_unnorm = np.sqrt(mse_unnorm)\n",
    "mae_unnorm = mean_absolute_error(test_y_unnorm, predictions_unnorm)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Test Set (Normalized): {mse_normalized:.6f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Test Set (Normalized): {rmse_normalized:.6f}\")\n",
    "print(f\"Mean Squared Error (MSE) on Test Set (Unnormalized): {mse_unnorm:.6f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Test Set (Unnormalized): {rmse_unnorm:.6f}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Test Set (Unnormalized): {mae_unnorm:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9dc567d-26cc-4d27-af6e-59fcca646ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNetwork(\n",
      "  (lstm): LSTM(3, 32, batch_first=True)\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.05, inplace=False)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da4621-983c-41ae-95f4-5ccca9ab6271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
